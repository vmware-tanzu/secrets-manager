---
# Source: spire/templates/namespaces.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: spire-system
  labels:
    pod-security.kubernetes.io/audit: privileged
    pod-security.kubernetes.io/enforce: privileged
    pod-security.kubernetes.io/warn: privileged
    security.openshift.io/scc.podSecurityLabelSync: "false"
---
# Source: spire/templates/namespaces.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: spire-server
  labels:
    pod-security.kubernetes.io/audit: restricted
    pod-security.kubernetes.io/enforce: privileged
    pod-security.kubernetes.io/warn: restricted
    security.openshift.io/scc.podSecurityLabelSync: "false"
---
# Source: spire/charts/spiffe-csi-driver/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: spire-spiffe-csi-driver
  namespace: spire-system
  labels:
    helm.sh/chart: spiffe-csi-driver-0.1.0
    app.kubernetes.io/name: spiffe-csi-driver
    app.kubernetes.io/instance: spire
    app.kubernetes.io/version: "0.2.3"
    app.kubernetes.io/managed-by: Helm
---
# Source: spire/charts/spiffe-oidc-discovery-provider/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: spire-spiffe-oidc-discovery-provider
  namespace: spire-server
  labels:
    helm.sh/chart: spiffe-oidc-discovery-provider-0.1.0
    app.kubernetes.io/name: spiffe-oidc-discovery-provider
    app.kubernetes.io/instance: spire
    app.kubernetes.io/version: "1.9.6"
    app.kubernetes.io/managed-by: Helm
---
# Source: spire/charts/spire-agent/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: spire-agent
  namespace: spire-system
  labels:
    helm.sh/chart: spire-agent-0.1.0
    app.kubernetes.io/name: agent
    app.kubernetes.io/instance: spire
    app.kubernetes.io/version: "1.9.6"
    app.kubernetes.io/managed-by: Helm
---
# Source: spire/charts/spire-server/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: spire-server
  namespace: spire-server
  labels:
    helm.sh/chart: spire-server-0.1.0
    app.kubernetes.io/name: server
    app.kubernetes.io/instance: spire
    app.kubernetes.io/version: "1.9.6"
    app.kubernetes.io/managed-by: Helm
---
# Source: spire/charts/spiffe-oidc-discovery-provider/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: spire-spiffe-oidc-discovery-provider
  namespace: spire-server
data:
  oidc-discovery-provider.conf: |
    {
      "domains": [
        "spire-spiffe-oidc-discovery-provider",
        "spire-spiffe-oidc-discovery-provider.spire-server",
        "spire-spiffe-oidc-discovery-provider.spire-server.svc.cluster.local",
        "oidc-discovery.aegis.ist"
      ],
      "health_checks": {
        "bind_port": "8008",
        "live_path": "/live",
        "ready_path": "/ready"
      },
      "log_level": "info",
      "serving_cert_file": {
        "addr": ":8443",
        "cert_file_path": "/certs/tls.crt",
        "key_file_path": "/certs/tls.key"
      },
      "workload_api": {
        "socket_path": "/spiffe-workload-api/spire-agent.sock",
        "trust_domain": "aegis.ist"
      }
    }
  spiffe-helper.conf: |
    agent_address = "/spiffe-workload-api/spire-agent.sock"
    cert_dir = "/certs"
    svid_file_name = "tls.crt"
    svid_key_file_name = "tls.key"
    svid_bundle_file_name = "ca.pem"
---
# Source: spire/charts/spire-agent/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: spire-agent
  namespace: spire-system
data:
  agent.conf: |
    {
      "agent": {
        "data_dir": "/run/spire",
        "log_level": "info",
        "retry_bootstrap": true,
        "server_address": "spire-server.spire-server",
        "server_port": "443",
        "socket_path": "/tmp/spire-agent/public/spire-agent.sock",
        "trust_bundle_path": "/run/spire/bundle/bundle.crt",
        "trust_domain": "aegis.ist"
      },
      "health_checks": {
        "bind_address": "0.0.0.0",
        "bind_port": "9982",
        "listener_enabled": true,
        "live_path": "/live",
        "ready_path": "/ready"
      },
      "plugins": {
        "KeyManager": [
          {
            "memory": {
              "plugin_data": null
            }
          }
        ],
        "NodeAttestor": [
          {
            "k8s_psat": {
              "plugin_data": {
                "cluster": "vsecm-cluster"
              }
            }
          }
        ],
        "WorkloadAttestor": [
          {
            "k8s": {
              "plugin_data": {
                "disable_container_selectors": false,
                "skip_kubelet_verification": true,
                "use_new_container_locator": false,
                "verbose_container_locator_logs": false
              }
            }
          }
        ]
      },
      "telemetry": [
        {
          "Prometheus": [
            {
              "host": "0.0.0.0",
              "port": 9988
            }
          ]
        }
      ]
    }
---
# Source: spire/charts/spire-server/templates/bundle-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: spire-bundle
  namespace: spire-system
---
# Source: spire/charts/spire-server/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: spire-server
  namespace: spire-server
data:
  server.conf: |
    {
      "health_checks": {
        "bind_address": "0.0.0.0",
        "bind_port": "8080",
        "listener_enabled": true,
        "live_path": "/live",
        "ready_path": "/ready"
      },
      "plugins": {
        "DataStore": [
          {
            "sql": {
              "plugin_data": {
                "connection_string": "/run/spire/data/datastore.sqlite3",
                "database_type": "sqlite3"
              }
            }
          }
        ],
        "KeyManager": [
          {
            "disk": {
              "plugin_data": {
                "keys_path": "/run/spire/data/keys.json"
              }
            }
          }
        ],
        "NodeAttestor": [
          {
            "k8s_psat": {
              "plugin_data": {
                "clusters": [
                  {
                    "vsecm-cluster": {
                      "allowed_node_label_keys": [],
                      "allowed_pod_label_keys": [],
                      "audience": [
                        "spire-server"
                      ],
                      "service_account_allow_list": [
                        "spire-system:spire-agent"
                      ]
                    }
                  }
                ]
              }
            }
          }
        ],
        "Notifier": [
          {
            "k8sbundle": {
              "plugin_data": {
                "config_map": "spire-bundle",
                "namespace": "spire-system"
              }
            }
          }
        ]
      },
      "server": {
        "audit_log_enabled": false,
        "bind_address": "0.0.0.0",
        "bind_port": "8081",
        "ca_key_type": "rsa-2048",
        "ca_subject": [
          {
            "common_name": "aegist.ist",
            "country": [
              "US"
            ],
            "organization": [
              "aegis.ist"
            ]
          }
        ],
        "ca_ttl": "24h",
        "data_dir": "/run/spire/data",
        "default_jwt_svid_ttl": "1h",
        "default_x509_svid_ttl": "4h",
        "jwt_issuer": "https://oidc-discovery.aegis.ist",
        "log_level": "info",
        "trust_domain": "aegis.ist"
      },
      "telemetry": [
        {
          "Prometheus": [
            {
              "host": "0.0.0.0",
              "port": 9988
            }
          ]
        }
      ]
    }
---
# Source: spire/charts/spire-server/templates/controller-manager-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: spire-controller-manager
  namespace: spire-server
data:
  controller-manager-config.yaml: |
    
    apiVersion: spire.spiffe.io/v1alpha1
    kind: ControllerManagerConfig
    metadata:
      name: spire-controller-manager
      namespace: spire-server
      labels:
        helm.sh/chart: spire-server-0.1.0
        app.kubernetes.io/name: server
        app.kubernetes.io/instance: spire
        app.kubernetes.io/version: "1.9.6"
        app.kubernetes.io/managed-by: Helm
    metrics:
      bindAddress: 0.0.0.0:8082
    health:
      healthProbeBindAddress: 0.0.0.0:8083
    leaderElection:
      leaderElect: true
      resourceName: 6f304bd2.spiffe.io
      resourceNamespace: spire-server
    validatingWebhookConfigurationName: spire-server-spire-controller-manager-webhook
    entryIDPrefix: vsecm-cluster
    clusterName: vsecm-cluster
    trustDomain: aegis.ist
    ignoreNamespaces:
      - kube-system
      - kube-public
      - local-path-storage
      - openshift-cluster-node-tuning-operator
      - openshift-cluster-samples-operator
      - openshift-cluster-storage-operator
      - openshift-console-operator
      - openshift-console
      - openshift-dns
      - openshift-dns-operator
      - openshift-image-registry
      - openshift-ingress
      - openshift-kube-storage-version-migrator
      - openshift-kube-storage-version-migrator-operator
      - openshift-kube-proxy
      - openshift-marketplace
      - openshift-monitoring
      - openshift-multus
      - openshift-network-diagnostics
      - openshift-network-operator
      - openshift-operator-lifecycle-manager
      - openshift-roks-metrics
      - openshift-service-ca-operator
      - openshift-service-ca
      - ibm-odf-validation-webhook
      - ibm-system
    spireServerSocketPath: "/tmp/spire-server/private/api.sock"
    className: "spire-server-spire"
    watchClassless: false
    parentIDTemplate: "spiffe://{{ .TrustDomain }}/spire/agent/k8s_psat/{{ .ClusterName }}/{{ .NodeMeta.UID }}"
    reconcile:
      clusterSPIFFEIDs: true
      clusterStaticEntries: true
      clusterFederatedTrustDomains: true
---
# Source: spire/charts/spire-agent/templates/roles.yaml
# Required cluster role to allow spire-agent to query k8s API server
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: spire-agent
rules:
  - apiGroups: [""]
    resources:
      - pods
      - nodes
      - nodes/proxy
    verbs: ["get"]
---
# Source: spire/charts/spire-server/templates/controller-manager-roles.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: spire-server-spire-controller-manager
rules:
  - apiGroups: [""]
    resources: ["namespaces"]
    verbs: ["get", "list", "watch"]
  - apiGroups: ["admissionregistration.k8s.io"]
    resources: ["validatingwebhookconfigurations"]
    verbs: ["get", "list", "patch", "watch"]
  - apiGroups: [""]
    resources: ["nodes"]
    verbs: ["get", "list", "watch"]
  - apiGroups: [""]
    resources: ["endpoints"]
    verbs: ["get", "list", "watch"]
  - apiGroups: [""]
    resources: ["pods"]
    verbs: ["get", "list", "watch"]
  - apiGroups: ["spire.spiffe.io"]
    resources: ["clusterfederatedtrustdomains"]
    verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
  - apiGroups: ["spire.spiffe.io"]
    resources: ["clusterfederatedtrustdomains/finalizers"]
    verbs: ["update"]
  - apiGroups: ["spire.spiffe.io"]
    resources: ["clusterfederatedtrustdomains/status"]
    verbs: ["get", "patch", "update"]
  - apiGroups: ["spire.spiffe.io"]
    resources: ["clusterspiffeids"]
    verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
  - apiGroups: ["spire.spiffe.io"]
    resources: ["clusterspiffeids/finalizers"]
    verbs: ["update"]
  - apiGroups: ["spire.spiffe.io"]
    resources: ["clusterspiffeids/status"]
    verbs: ["get", "patch", "update"]
  - apiGroups: ["spire.spiffe.io"]
    resources: ["clusterstaticentries"]
    verbs: ["create", "delete", "get", "list", "patch", "update", "watch"]
  - apiGroups: ["spire.spiffe.io"]
    resources: ["clusterstaticentries/finalizers"]
    verbs: ["update"]
  - apiGroups: ["spire.spiffe.io"]
    resources: ["clusterstaticentries/status"]
    verbs: ["get", "patch", "update"]
---
# Source: spire/charts/spire-server/templates/roles.yaml
# ClusterRole to allow spire-server node attestor to query Token Review API
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: spire-server-spire-server
rules:
  - apiGroups: [authentication.k8s.io]
    resources: [tokenreviews]
    verbs:
      - get
      - watch
      - list
      - create
  - apiGroups: [""]
    resources: [nodes, pods]
    verbs:
      - get
      - list
---
# Source: spire/charts/spire-agent/templates/roles.yaml
# Binds above cluster role to spire-agent service account
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: spire-agent
subjects:
  - kind: ServiceAccount
    name: spire-agent
    namespace: spire-system
roleRef:
  kind: ClusterRole
  name: spire-agent
  apiGroup: rbac.authorization.k8s.io
---
# Source: spire/charts/spire-server/templates/controller-manager-roles.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: spire-server-spire-controller-manager
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: spire-server-spire-controller-manager

subjects:
- kind: ServiceAccount
  name: spire-server
  namespace: spire-server
---
# Source: spire/charts/spire-server/templates/roles.yaml
# Binds above cluster role to spire-server service account
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: spire-server-spire-server

subjects:
- kind: ServiceAccount
  name: spire-server
  namespace: spire-server
roleRef:
  kind: ClusterRole
  name: spire-server-spire-server
  apiGroup: rbac.authorization.k8s.io
---
# Source: spire/charts/spire-server/templates/controller-manager-roles.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: spire-controller-manager-leader-election
  namespace: spire-server
rules:
  - apiGroups: [""]
    resources: ["configmaps"]
    verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
  - apiGroups: ["coordination.k8s.io"]
    resources: ["leases"]
    verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
  - apiGroups: [""]
    resources: ["events"]
    verbs: ["create", "patch"]
---
# Source: spire/charts/spire-server/templates/roles.yaml
# Role to be able to push certificate bundles to a configmap
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: spire-bundle
  namespace: spire-system
rules:
  - apiGroups: [""]
    resources: [configmaps]
    resourceNames: [spire-bundle]
    verbs:
      - get
      - patch
---
# Source: spire/charts/spire-server/templates/controller-manager-roles.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: spire-controller-manager-leader-election
  namespace: spire-server
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: spire-controller-manager-leader-election

subjects:
- kind: ServiceAccount
  name: spire-server
  namespace: spire-server
---
# Source: spire/charts/spire-server/templates/roles.yaml
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: spire-bundle
  namespace: spire-system

subjects:
- kind: ServiceAccount
  name: spire-server
  namespace: spire-server
roleRef:
  kind: Role
  name: spire-bundle
  apiGroup: rbac.authorization.k8s.io
---
# Source: spire/charts/spiffe-oidc-discovery-provider/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: spire-spiffe-oidc-discovery-provider
  namespace: spire-server
spec:
  type: ClusterIP
  ports:
    - name: https
      port: 443
      targetPort: https
      protocol: TCP
  selector:
    app.kubernetes.io/name: spiffe-oidc-discovery-provider
    app.kubernetes.io/instance: spire
---
# Source: spire/charts/spire-server/templates/controller-manager-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: spire-controller-manager-webhook
  namespace: spire-server
  labels:
    helm.sh/chart: spire-server-0.1.0
    app.kubernetes.io/name: server
    app.kubernetes.io/instance: spire
    app.kubernetes.io/version: "1.9.6"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - name: https
      port: 443
      targetPort: https
      protocol: TCP
  selector:
    app.kubernetes.io/name: server
    app.kubernetes.io/instance: spire
---
# Source: spire/charts/spire-server/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: spire-server
  namespace: spire-server
  labels:
    helm.sh/chart: spire-server-0.1.0
    app.kubernetes.io/name: server
    app.kubernetes.io/instance: spire
    app.kubernetes.io/version: "1.9.6"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - name: grpc
      port: 443
      targetPort: grpc
      protocol: TCP
  selector:
    app.kubernetes.io/name: server
    app.kubernetes.io/instance: spire
---
# Source: spire/charts/spiffe-csi-driver/templates/daemonset.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: spire-spiffe-csi-driver
  namespace: spire-system
  labels:
    helm.sh/chart: spiffe-csi-driver-0.1.0
    app.kubernetes.io/name: spiffe-csi-driver
    app.kubernetes.io/instance: spire
    app.kubernetes.io/version: "0.2.3"
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: spiffe-csi-driver
      app.kubernetes.io/instance: spire
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
  template:
    metadata:
      labels:
        app.kubernetes.io/name: spiffe-csi-driver
        app.kubernetes.io/instance: spire
    spec:
      serviceAccountName: spire-spiffe-csi-driver
      
      priorityClassName: system-node-critical
      initContainers:
        - name: set-context
          command:
            - chcon
            - '-Rvt'
            - container_file_t
            - spire-agent-socket/
          image: registry.access.redhat.com/ubi9:latest
          imagePullPolicy: Always
          securityContext:
            capabilities:
              drop:
                - all
            privileged: true
          volumeMounts:
            - name: spire-agent-socket-dir
              mountPath: /spire-agent-socket
          terminationMessagePolicy: File
          terminationMessagePath: /dev/termination-log
      containers:
        # This is the container which runs the SPIFFE CSI driver.
        - name: spiffe-csi-driver
          image: ghcr.io/spiffe/spiffe-csi-driver:0.2.3
          imagePullPolicy: IfNotPresent
          args: [
            "-workload-api-socket-dir", "/spire-agent-socket",
            "-plugin-name", "csi.spiffe.io",
            "-csi-socket-path", "/spiffe-csi/csi.sock",
          ]
          env:
            # The CSI driver needs a unique node ID. The node name can be
            # used for this purpose.
            - name: MY_NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
          volumeMounts:
            # The volume containing the SPIRE agent socket. The SPIFFE CSI
            # driver will mount this directory into containers.
            - mountPath: /spire-agent-socket
              name: spire-agent-socket-dir
              readOnly: true
            # The volume that will contain the CSI driver socket shared
            # with the kubelet and the driver registrar.
            - mountPath: /spiffe-csi
              name: spiffe-csi-socket-dir
            # The volume containing mount points for containers.
            - mountPath: /var/lib/kubelet/pods
              mountPropagation: Bidirectional
              name: mountpoint-dir
          securityContext:
            readOnlyRootFilesystem: true
            capabilities:
              drop:
                - all
            privileged: true
          resources:
            {}
        # This container runs the CSI Node Driver Registrar which takes care
        # of all the little details required to register a CSI driver with
        # the kubelet.
        - name: node-driver-registrar
          image: registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.9.4
          imagePullPolicy: IfNotPresent
          args: [
            "-csi-address", "/spiffe-csi/csi.sock",
            "-kubelet-registration-path", "/var/lib/kubelet/plugins/csi.spiffe.io/csi.sock",
            "-health-port", "9809"
          ]
          volumeMounts:
            # The registrar needs access to the SPIFFE CSI driver socket
            - mountPath: /spiffe-csi
              name: spiffe-csi-socket-dir
            # The registrar needs access to the Kubelet plugin registration
            # directory
            - name: kubelet-plugin-registration-dir
              mountPath: /registration
          ports:
            - containerPort: 9809
              name: healthz
          livenessProbe:
            httpGet:
              path: /healthz
              port: healthz
            initialDelaySeconds: 5
            timeoutSeconds: 5
          resources:
            {}
      volumes:
        - name: spire-agent-socket-dir
          hostPath:
            path: /run/spire/agent-sockets
            type: DirectoryOrCreate
        # This volume is where the socket for kubelet->driver communication lives
        - name: spiffe-csi-socket-dir
          hostPath:
            path: /var/lib/kubelet/plugins/csi.spiffe.io
            type: DirectoryOrCreate
        # This volume is where the SPIFFE CSI driver mounts volumes
        - name: mountpoint-dir
          hostPath:
            path: /var/lib/kubelet/pods
            type: Directory
        # This volume is where the node-driver-registrar registers the plugin
        # with kubelet
        - name: kubelet-plugin-registration-dir
          hostPath:
            path: /var/lib/kubelet/plugins_registry
            type: Directory
---
# Source: spire/charts/spire-agent/templates/daemonset.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: spire-agent
  namespace: spire-system
  labels:
    helm.sh/chart: spire-agent-0.1.0
    app.kubernetes.io/name: agent
    app.kubernetes.io/instance: spire
    app.kubernetes.io/version: "1.9.6"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: default
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: agent
      app.kubernetes.io/instance: spire
      app.kubernetes.io/component: default
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
  template:
    metadata:
      annotations:
        kubectl.kubernetes.io/default-container: spire-agent
        checksum/config: 2ad907b85aad20064f4cbf04be0f3bf500bbe6a43f76c82c48eda97306352008
      labels:
        app.kubernetes.io/name: agent
        app.kubernetes.io/instance: spire
        app.kubernetes.io/component: default
    spec:
      hostPID: true
      hostNetwork: true
      dnsPolicy: ClusterFirstWithHostNet
      serviceAccountName: spire-agent
      securityContext:
        fsGroupChangePolicy: OnRootMismatch
      
      priorityClassName: system-node-critical
      initContainers:
        - name: ensure-alternate-names
          image: cgr.dev/chainguard/bash:latest@sha256:8c9e5cbb641ced8112c637eb3611dab29bf65448a9d884a03938baf1b352dc4d
          imagePullPolicy: Always
          command: ["bash", "-xc"]
          args:
            - |
              cd /run/spire/agent-sockets
              L=`readlink socket`
              [ "x$L" != "xspire-agent.sock" ] && rm -f socket
              [ ! -L socket ] && ln -s spire-agent.sock socket
              L=`readlink api.sock`
              [ "x$L" != "xspire-agent.sock" ] && rm -f api.sock
              [ ! -L api.sock ] && ln -s spire-agent.sock api.sock
              [ -L spire-agent.sock ] && rm -f spire-agent.sock
              exit 0
          resources:
            {}
          volumeMounts:
            - name: spire-agent-socket-dir
              mountPath: /run/spire/agent-sockets
          securityContext:
            runAsUser: 0
            runAsGroup: 0
      containers:
        - name: spire-agent
          image: ghcr.io/spiffe/spire-agent:1.9.6
          imagePullPolicy: IfNotPresent
          args: ["-config", "/opt/spire/conf/agent/agent.conf"]
          securityContext:
            {}
          env:
            - name: PATH
              value: "/opt/spire/bin:/bin"
            - name: MY_NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
          ports:
            - containerPort: 9982
              name: healthz
            - containerPort: 9988
              name: prom
          volumeMounts:
            - name: spire-config
              mountPath: /opt/spire/conf/agent
              readOnly: true
            - name: spire-bundle
              mountPath: /run/spire/bundle
              readOnly: true
            - name: spire-agent-socket-dir
              mountPath: /tmp/spire-agent/public
              readOnly: false
            - name: spire-token
              mountPath: /var/run/secrets/tokens
          livenessProbe:
            httpGet:
              path: /live
              port: healthz
            initialDelaySeconds: 15
            periodSeconds: 60
          readinessProbe:
            httpGet:
              path: /ready
              port: healthz
            initialDelaySeconds: 10
            periodSeconds: 30
          resources:
            {}
      volumes:
        - name: spire-config
          configMap:
            name: spire-agent
        - name: spire-agent-admin-socket-dir
          emptyDir: {}
        - name: spire-bundle
          configMap:
            name: spire-bundle
        - name: spire-token
          projected:
            sources:
            - serviceAccountToken:
                path: spire-agent
                expirationSeconds: 7200
                audience: spire-server
        - name: spire-agent-socket-dir
          hostPath:
            path: /run/spire/agent-sockets
            type: DirectoryOrCreate
---
# Source: spire/charts/spiffe-oidc-discovery-provider/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: spire-spiffe-oidc-discovery-provider
  namespace: spire-server
  labels:
    helm.sh/chart: spiffe-oidc-discovery-provider-0.1.0
    app.kubernetes.io/name: spiffe-oidc-discovery-provider
    app.kubernetes.io/instance: spire
    app.kubernetes.io/version: "1.9.6"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: spiffe-oidc-discovery-provider
      app.kubernetes.io/instance: spire
  template:
    metadata:
      annotations:
        checksum/config: 856b450a332226fc0b9ea4c2145d8234ebce9220ad5239134629ac0c1cbb63ba
      labels:
        app.kubernetes.io/name: spiffe-oidc-discovery-provider
        app.kubernetes.io/instance: spire
        release: spire
        release-namespace: spire-server
        component: oidc-discovery-provider
    spec:
      serviceAccountName: spire-spiffe-oidc-discovery-provider
      securityContext:
        fsGroupChangePolicy: OnRootMismatch
      initContainers:
        - name: init
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            seccompProfile:
              type: RuntimeDefault
          resources:
            {}
          image: ghcr.io/spiffe/spiffe-helper:nightly@sha256:8cee346ffdcee5c996d394f1c3bb761c2c06834a0e779a78db6dc6a46fd13ae6
          imagePullPolicy: IfNotPresent
          args:
            - -config
            - /etc/spiffe-helper.conf
            - -exitWhenReady
          volumeMounts:
            - name: spiffe-workload-api
              mountPath: /spiffe-workload-api
              readOnly: true
            - name: spire-oidc-config
              mountPath: /etc/spiffe-helper.conf
              subPath: spiffe-helper.conf
              readOnly: true
            - name: certdir
              mountPath: /certs
      containers:
        - name: spiffe-oidc-discovery-provider
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            seccompProfile:
              type: RuntimeDefault
          image: ghcr.io/spiffe/oidc-discovery-provider:1.9.6
          imagePullPolicy: IfNotPresent
          args:
            - -config
            - /run/spire/oidc/config/oidc-discovery-provider.conf
          ports:
            - containerPort: 8008
              name: healthz
            - containerPort: 8443
              name: https
          volumeMounts:
            - name: spiffe-workload-api
              mountPath: /spiffe-workload-api
              readOnly: true
            - name: spire-oidc-sockets
              mountPath: /run/spire/oidc-sockets
              readOnly: false
            - name: spire-oidc-config
              mountPath: /run/spire/oidc/config/oidc-discovery-provider.conf
              subPath: oidc-discovery-provider.conf
              readOnly: true
            - name: certdir
              mountPath: /certs
              readOnly: true
          readinessProbe:
            httpGet:
              path: /ready
              port: healthz
            initialDelaySeconds: 5
            periodSeconds: 5
          livenessProbe:
            httpGet:
              path: /live
              port: healthz
            initialDelaySeconds: 5
            periodSeconds: 5
          resources:
            {}
        - name: spiffe-helper
          resources:
            {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            seccompProfile:
              type: RuntimeDefault
          image: ghcr.io/spiffe/spiffe-helper:nightly@sha256:8cee346ffdcee5c996d394f1c3bb761c2c06834a0e779a78db6dc6a46fd13ae6
          imagePullPolicy: IfNotPresent
          args:
            - -config
            - /etc/spiffe-helper.conf
          volumeMounts:
            - name: spiffe-workload-api
              mountPath: /spiffe-workload-api
              readOnly: true
            - name: spire-oidc-config
              mountPath: /etc/spiffe-helper.conf
              subPath: spiffe-helper.conf
              readOnly: true
            - name: certdir
              mountPath: /certs
      volumes:
        - name: spiffe-workload-api
          csi:
            driver: "csi.spiffe.io"
            readOnly: true
        - name: spire-oidc-sockets
          emptyDir: {}
        - name: spire-oidc-config
          configMap:
            name: spire-spiffe-oidc-discovery-provider
        - name: nginx-tmp
          emptyDir: {}
        - name: certdir
          emptyDir: {}
---
# Source: spire/charts/spire-server/templates/server-resource.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: spire-server
  namespace: spire-server
  labels:
    helm.sh/chart: spire-server-0.1.0
    app.kubernetes.io/name: server
    app.kubernetes.io/instance: spire
    app.kubernetes.io/version: "1.9.6"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: server
spec:
  replicas: 1
  serviceName: spire-server
  selector:
    matchLabels:
      app.kubernetes.io/name: server
      app.kubernetes.io/instance: spire
      app.kubernetes.io/component: server
  template:
    metadata:
      annotations:
        kubectl.kubernetes.io/default-container: spire-server
        checksum/config: 83dddc7bb9f54b5059533228971826c0585045b7c4afb17635ede1e7ef6c1e35
        checksum/config2: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
        checksum/config3: 9742ccbbd63b5da94e50bc34b73c946f254110b1f94fbc4ac437b3bba15cefe8
        checksum/configTornjak: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
      labels:
        app.kubernetes.io/name: server
        app.kubernetes.io/instance: spire
        app.kubernetes.io/component: server
        component: server
        release: spire
        release-namespace: spire-server
    spec:
      serviceAccountName: spire-server
      shareProcessNamespace: true
      securityContext:
        fsGroupChangePolicy: OnRootMismatch
      
      priorityClassName: system-cluster-critical
      containers:
        - name: spire-server
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            seccompProfile:
              type: RuntimeDefault
          image: ghcr.io/spiffe/spire-server:1.9.6
          imagePullPolicy: IfNotPresent
          args:
            - -expandEnv
            - -config
            - /run/spire/config/server.conf
          env:
          - name: PATH
            value: "/opt/spire/bin:/bin"
          ports:
            - name: grpc
              containerPort: 8081
              protocol: TCP
            - containerPort: 8080
              name: healthz
            - containerPort: 9988
              name: prom
          livenessProbe:
            httpGet:
              path: /live
              port: healthz
            failureThreshold: 2
            initialDelaySeconds: 15
            periodSeconds: 60
            timeoutSeconds: 3
          readinessProbe:
            httpGet:
              path: /ready
              port: healthz
            initialDelaySeconds: 5
            periodSeconds: 5
          resources:
            {}
          volumeMounts:
            - name: spire-server-socket
              mountPath: /tmp/spire-server/private
              readOnly: false
            - name: spire-config
              mountPath: /run/spire/config
              readOnly: true
            - name: spire-data
              mountPath: /run/spire/data
              readOnly: false
            - name: server-tmp
              mountPath: /tmp
              readOnly: false
        
        - name: spire-controller-manager
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            seccompProfile:
              type: RuntimeDefault
          image: ghcr.io/spiffe/spire-controller-manager:0.5.0
          imagePullPolicy: IfNotPresent
          args:
            - --config=controller-manager-config.yaml
          env:
            - name: ENABLE_WEBHOOKS
              value: "true"
          ports:
            - name: https
              containerPort: 9443
              protocol: TCP
            - containerPort: 8083
              name: healthz
            - containerPort: 8082
              name: prom-cm
          livenessProbe:
            httpGet:
              path: /healthz
              port: healthz
          readinessProbe:
            httpGet:
              path: /readyz
              port: healthz
          resources:
            {}
          volumeMounts:
            - name: spire-server-socket
              mountPath: /tmp/spire-server/private
              readOnly: true
            - name: controller-manager-config
              mountPath: /controller-manager-config.yaml
              subPath: controller-manager-config.yaml
              readOnly: true
            - name: spire-controller-manager-tmp
              mountPath: /tmp
              subPath: spire-controller-manager
              readOnly: false
      volumes:
        - name: server-tmp
          emptyDir: {}
        - name: spire-config
          configMap:
            name: spire-server
        - name: spire-server-socket
          emptyDir: {}
        - name: spire-controller-manager-tmp
          emptyDir: {}
        - name: controller-manager-config
          configMap:
            name: spire-controller-manager
  volumeClaimTemplates:
    - metadata:
        name: spire-data
      spec:
        accessModes:
          - ReadWriteOnce
        resources:
          requests:
            storage: 1Gi
---
# Source: spire/charts/spiffe-csi-driver/templates/spiffe-csi-driver.yaml
apiVersion: storage.k8s.io/v1
kind: CSIDriver
metadata:
  name: "csi.spiffe.io"
  labels:
    security.openshift.io/csi-ephemeral-volume-profile: restricted

spec:
  # Only ephemeral, inline volumes are supported. There is no need for a
  # controller to provision and attach volumes.
  attachRequired: false

  # Request the pod information which the CSI driver uses to verify that an
  # ephemeral mount was requested.
  podInfoOnMount: true

  # Don't change ownership on the contents of the mount since the Workload API
  # Unix Domain Socket is typically open to all (i.e. 0777).
  fsGroupPolicy: None

  # Declare support for ephemeral volumes only.
  volumeLifecycleModes:
    - Ephemeral
---
# Source: spire/charts/spire-server/templates/controller-manager-cluster-ids.yaml
apiVersion: spire.spiffe.io/v1alpha1
kind: ClusterSPIFFEID
metadata:
  name: spire-server-spire-default
spec:
  className: "spire-server-spire"
  spiffeIDTemplate: "spiffe://{{ .TrustDomain }}/ns/{{ .PodMeta.Namespace }}/sa/{{ .PodSpec.ServiceAccountName }}"
  namespaceSelector:
    matchExpressions:
    - key: kubernetes.io/metadata.name
      operator: NotIn
      values:
      - spire-server
      - spire-system
---
# Source: spire/charts/spire-server/templates/controller-manager-cluster-ids.yaml
apiVersion: spire.spiffe.io/v1alpha1
kind: ClusterSPIFFEID
metadata:
  name: spire-server-spire-oidc-discovery-provider
spec:
  className: "spire-server-spire"
  spiffeIDTemplate: "spiffe://{{ .TrustDomain }}/ns/{{ .PodMeta.Namespace }}/sa/{{ .PodSpec.ServiceAccountName }}"
  podSelector:
    matchLabels:
      component: oidc-discovery-provider
      release: spire
      release-namespace: spire-server
  namespaceSelector:
    matchExpressions:
    - key: kubernetes.io/metadata.name
      operator: In
      values:
      - spire-server
      - spire-system
  dnsNameTemplates:
    - oidc-discovery.{{ .TrustDomain }}
  autoPopulateDNSNames: true
---
# Source: spire/charts/spire-server/templates/controller-manager-cluster-ids.yaml
apiVersion: spire.spiffe.io/v1alpha1
kind: ClusterSPIFFEID
metadata:
  name: spire-server-spire-test-keys
spec:
  className: "spire-server-spire"
  spiffeIDTemplate: "spiffe://{{ .TrustDomain }}/ns/{{ .PodMeta.Namespace }}/sa/{{ .PodSpec.ServiceAccountName }}"
  podSelector:
    matchLabels:
      component: test-keys
      release: spire
      release-namespace: spire-server
  namespaceSelector:
    matchExpressions:
    - key: kubernetes.io/metadata.name
      operator: In
      values:
      - spire-server
      - spire-system
---
# Source: spire/charts/spiffe-csi-driver/templates/scc-spiffe-csi-driver.yaml
apiVersion: security.openshift.io/v1
kind: SecurityContextConstraints
metadata:
  name: spire-spiffe-csi-driver
readOnlyRootFilesystem: true
runAsUser:
  type: RunAsAny
seLinuxContext:
  type: RunAsAny
supplementalGroups:
  type: RunAsAny
users:
  - system:serviceaccount:spire-system:spire-spiffe-csi-driver
volumes:
  - configmap
  - hostPath
  - secret
allowHostDirVolumePlugin: true
allowHostIPC: false
allowHostNetwork: false
allowHostPID: false
allowHostPorts: false
allowPrivilegeEscalation: true
allowPrivilegedContainer: true
fsGroup:
  type: RunAsAny
groups: []
---
# Source: spire/charts/spiffe-oidc-discovery-provider/templates/scc-spire-oidc-discovery-provider.yaml
apiVersion: security.openshift.io/v1
kind: SecurityContextConstraints
metadata:
  name: spire-spiffe-oidc-discovery-provider
readOnlyRootFilesystem: true
runAsUser:
  type: RunAsAny
seLinuxContext:
  type: RunAsAny
supplementalGroups:
  type: RunAsAny
users:
  - system:serviceaccount:spire-server:spire-spiffe-oidc-discovery-provider
  - system:serviceaccount:spire-server:spire-spiffe-oidc-discovery-provider-pre-delete
volumes:
  - configMap
  - csi
  - downwardAPI
  - emptyDir
  - ephemeral
  - hostPath
  - projected
  - secret
allowHostDirVolumePlugin: true
allowHostIPC: true
allowHostNetwork: true
allowHostPID: true
allowHostPorts: true
allowPrivilegeEscalation: true
allowPrivilegedContainer: true
fsGroup:
  type: RunAsAny
groups: []
seccompProfiles:
  - '*'
---
# Source: spire/charts/spire-agent/templates/scc-spire-agent.yaml
apiVersion: security.openshift.io/v1
kind: SecurityContextConstraints
metadata:
  name: spire-agent
readOnlyRootFilesystem: true
runAsUser:
  type: RunAsAny
seLinuxContext:
  type: RunAsAny
supplementalGroups:
  type: RunAsAny
users:
  - system:serviceaccount:spire-system:spire-agent
volumes:
  - configMap
  - hostPath
  - projected
  - secret
  - emptyDir
allowHostDirVolumePlugin: true
allowHostIPC: true
allowHostNetwork: true
allowHostPID: true
allowHostPorts: true
allowPrivilegeEscalation: true
allowPrivilegedContainer: true
fsGroup:
  type: RunAsAny
groups: []
---
# Source: spire/charts/spire-server/templates/controller-manager-webhook.yaml
apiVersion: admissionregistration.k8s.io/v1
kind: ValidatingWebhookConfiguration
metadata:
  name: spire-server-spire-controller-manager-webhook
webhooks:
  - admissionReviewVersions: ["v1"]
    clientConfig:
      service:
        name: spire-controller-manager-webhook
        namespace: spire-server
        path: /validate-spire-spiffe-io-v1alpha1-clusterfederatedtrustdomain
    failurePolicy: Ignore # Actual value to be set by post install/upgrade hooks
    name: vclusterfederatedtrustdomain.kb.io
    rules:
      - apiGroups: ["spire.spiffe.io"]
        apiVersions: ["v1alpha1"]
        operations: ["CREATE", "UPDATE"]
        resources: ["clusterfederatedtrustdomains"]
    sideEffects: None
  - admissionReviewVersions: ["v1"]
    clientConfig:
      service:
        name: spire-controller-manager-webhook
        namespace: spire-server
        path: /validate-spire-spiffe-io-v1alpha1-clusterspiffeid
    failurePolicy: Ignore # Actual value to be set by post install/upgrade hooks
    name: vclusterspiffeid.kb.io
    rules:
      - apiGroups: ["spire.spiffe.io"]
        apiVersions: ["v1alpha1"]
        operations: ["CREATE", "UPDATE"]
        resources: ["clusterspiffeids"]
    sideEffects: None
---
# Source: spire/charts/spiffe-oidc-discovery-provider/templates/pre-delete-hook.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: spire-spiffe-oidc-discovery-provider-pre-delete
  namespace: spire-server
  labels:
    helm.sh/chart: spiffe-oidc-discovery-provider-0.1.0
    app.kubernetes.io/name: spiffe-oidc-discovery-provider
    app.kubernetes.io/instance: spire
    app.kubernetes.io/version: "1.9.6"
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": pre-delete
    "helm.sh/hook-delete-policy": before-hook-creation, hook-succeeded, hook-failed
---
# Source: spire/charts/spire-server/templates/post-install-hook.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: spire-server-post-install
  namespace: spire-server
  labels:
    helm.sh/chart: spire-server-0.1.0
    app.kubernetes.io/name: server
    app.kubernetes.io/instance: spire
    app.kubernetes.io/version: "1.9.6"
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": post-install
    "helm.sh/hook-delete-policy": before-hook-creation, hook-succeeded, hook-failed
---
# Source: spire/charts/spire-server/templates/post-upgrade-hook.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: spire-server-post-upgrade
  namespace: spire-server
  labels:
    helm.sh/chart: spire-server-0.1.0
    app.kubernetes.io/name: server
    app.kubernetes.io/instance: spire
    app.kubernetes.io/version: "1.9.6"
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": post-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation, hook-succeeded, hook-failed
---
# Source: spire/charts/spire-server/templates/pre-upgrade-hook.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: spire-server-pre-upgrade
  namespace: spire-server
  labels:
    helm.sh/chart: spire-server-0.1.0
    app.kubernetes.io/name: server
    app.kubernetes.io/instance: spire
    app.kubernetes.io/version: "1.9.6"
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": pre-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation, hook-succeeded, hook-failed
---
# Source: spire/charts/spire-server/templates/post-install-hook.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: spire-server-post-install
  annotations:
    "helm.sh/hook": post-install
    "helm.sh/hook-delete-policy": before-hook-creation, hook-succeeded, hook-failed
rules:
  - apiGroups: ["admissionregistration.k8s.io"]
    resources: ["validatingwebhookconfigurations"]
    resourceNames: ["spire-server-spire-controller-manager-webhook"]
    verbs: ["get", "patch"]
---
# Source: spire/charts/spire-server/templates/post-upgrade-hook.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: spire-server-post-upgrade
  annotations:
    "helm.sh/hook": post-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation, hook-succeeded, hook-failed
rules:
  - apiGroups: ["admissionregistration.k8s.io"]
    resources: ["validatingwebhookconfigurations"]
    resourceNames: ["spire-server-spire-controller-manager-webhook"]
    verbs: ["get", "patch"]
---
# Source: spire/charts/spire-server/templates/pre-upgrade-hook.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: spire-server-pre-upgrade
  annotations:
    "helm.sh/hook": pre-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation, hook-succeeded, hook-failed
rules:
  - apiGroups: ["admissionregistration.k8s.io"]
    resources: ["validatingwebhookconfigurations"]
    resourceNames: ["spire-server-spire-controller-manager-webhook"]
    verbs: ["get", "patch"]
---
# Source: spire/charts/spire-server/templates/post-install-hook.yaml
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: spire-server-post-install
  annotations:
    "helm.sh/hook": post-install
    "helm.sh/hook-delete-policy": before-hook-creation, hook-succeeded, hook-failed
subjects:
  - kind: ServiceAccount
    name: spire-server-post-install
    namespace: spire-server
roleRef:
  kind: ClusterRole
  name: spire-server-post-install
  apiGroup: rbac.authorization.k8s.io
---
# Source: spire/charts/spire-server/templates/post-upgrade-hook.yaml
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: spire-server-post-upgrade
  annotations:
    "helm.sh/hook": post-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation, hook-succeeded, hook-failed
subjects:
  - kind: ServiceAccount
    name: spire-server-post-upgrade
    namespace: spire-server
roleRef:
  kind: ClusterRole
  name: spire-server-post-upgrade
  apiGroup: rbac.authorization.k8s.io
---
# Source: spire/charts/spire-server/templates/pre-upgrade-hook.yaml
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: spire-server-pre-upgrade
  annotations:
    "helm.sh/hook": pre-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation, hook-succeeded, hook-failed
subjects:
  - kind: ServiceAccount
    name: spire-server-pre-upgrade
    namespace: spire-server
roleRef:
  kind: ClusterRole
  name: spire-server-pre-upgrade
  apiGroup: rbac.authorization.k8s.io
---
# Source: spire/charts/spiffe-oidc-discovery-provider/templates/pre-delete-hook.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: spire-spiffe-oidc-discovery-provider-pre-delete
  namespace: spire-server
  annotations:
    "helm.sh/hook": pre-delete
    "helm.sh/hook-delete-policy": before-hook-creation, hook-succeeded, hook-failed
rules:
  - apiGroups: ["apps"]
    resources: ["deployments"]
    resourceNames: ["spire-spiffe-oidc-discovery-provider"]
    verbs: ["get", "delete"]
---
# Source: spire/charts/spiffe-oidc-discovery-provider/templates/pre-delete-hook.yaml
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: spire-spiffe-oidc-discovery-provider-pre-delete
  namespace: spire-server
  annotations:
    "helm.sh/hook": pre-delete
    "helm.sh/hook-delete-policy": before-hook-creation, hook-succeeded, hook-failed
subjects:
  - kind: ServiceAccount
    name: spire-spiffe-oidc-discovery-provider-pre-delete
    namespace: spire-server
roleRef:
  kind: Role
  name: spire-spiffe-oidc-discovery-provider-pre-delete
  apiGroup: rbac.authorization.k8s.io
---
# Source: spire/charts/spiffe-oidc-discovery-provider/templates/tests/test-connection.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "spire-spiffe-oidc-discovery-provider-test-connection"
  namespace: spire-server
  labels:
    helm.sh/chart: spiffe-oidc-discovery-provider-0.1.0
    app.kubernetes.io/name: spiffe-oidc-discovery-provider
    app.kubernetes.io/instance: spire
    app.kubernetes.io/version: "1.9.6"
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": test
spec:
  securityContext:
    fsGroupChangePolicy: OnRootMismatch
  containers:
    - name: curl-service-name
      image: cgr.dev/chainguard/bash:latest@sha256:8c9e5cbb641ced8112c637eb3611dab29bf65448a9d884a03938baf1b352dc4d
      command: ['curl']
      args: ['-s', '-f', '-k', 'https://spire-spiffe-oidc-discovery-provider:443/.well-known/openid-configuration']
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
        runAsNonRoot: true
        seccompProfile:
          type: RuntimeDefault
    - name: curl-service-name-namespace
      image: cgr.dev/chainguard/bash:latest@sha256:8c9e5cbb641ced8112c637eb3611dab29bf65448a9d884a03938baf1b352dc4d
      command: ['curl']
      args: ['-s', '-f', '-k', 'https://spire-spiffe-oidc-discovery-provider.spire-server:443/.well-known/openid-configuration']
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
        runAsNonRoot: true
        seccompProfile:
          type: RuntimeDefault
    - name: curl-service-name-namespace-svc-cluster-local
      image: cgr.dev/chainguard/bash:latest@sha256:8c9e5cbb641ced8112c637eb3611dab29bf65448a9d884a03938baf1b352dc4d
      command: ['curl']
      args: ['-s', '-f', '-k', 'https://spire-spiffe-oidc-discovery-provider.spire-server.svc.cluster.local:443/.well-known/openid-configuration']
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
        runAsNonRoot: true
        seccompProfile:
          type: RuntimeDefault
  restartPolicy: Never
---
# Source: spire/charts/spiffe-oidc-discovery-provider/templates/tests/test-keys.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "spire-spiffe-oidc-discovery-provider-test-keys"
  namespace: spire-server
  labels:
    helm.sh/chart: spiffe-oidc-discovery-provider-0.1.0
    app.kubernetes.io/name: spiffe-oidc-discovery-provider
    app.kubernetes.io/instance: spire
    app.kubernetes.io/version: "1.9.6"
    app.kubernetes.io/managed-by: Helm
    release: spire
    release-namespace: spire-server
    component: test-keys
  annotations:
    "helm.sh/hook": test
spec:
  securityContext:
    fsGroupChangePolicy: OnRootMismatch
  serviceAccountName: spire-spiffe-oidc-discovery-provider
  initContainers:
    - name: static-busybox
      image: busybox:1.36.1-uclibc
      command:
        - sh
        - -c
        - |
          cp /bin/busybox /data/busybox
          chmod +x /data/busybox
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
        runAsNonRoot: true
        seccompProfile:
          type: RuntimeDefault
      volumeMounts:
        - name: data-volume
          mountPath: /data
    - name: install-step
      image: docker.io/smallstep/step-cli:0.26.1
      workingDir: /data
      command:
        - sh
        - -c
        - |
          cp /usr/local/bin/step /data/step
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
        runAsNonRoot: true
        seccompProfile:
          type: RuntimeDefault
      volumeMounts:
        - name: data-volume
          mountPath: /data
    - name: gettoken
      image: ghcr.io/spiffe/spire-agent:1.9.6
      command:
        - /data/busybox
        - sh
        - -c
        - |
          while true; do
            /opt/spire/bin/spire-agent api fetch jwt -audience foo -format json -socketPath /spire-agent/spire-agent.sock -timeout 5s > /data/token.svid
            [ $? -eq 0 ] && break
            sleep 1
          done
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
        runAsNonRoot: true
        seccompProfile:
          type: RuntimeDefault
      volumeMounts:
        - name: data-volume
          mountPath: /data
        - name: spire-api
          mountPath: /spire-agent
          readOnly: true
  containers:
    - name: verify-keys
      image: cgr.dev/chainguard/min-toolkit-debug:latest@sha256:d94454739d8be0239cfe93453df79c88d25d38b7a97084d81a49e9403a90d07c
      command:
        - bash
      workingDir: /data
      env:
      - name: TMPDIR
        value: /data
      args:
        - -cx
        - |
          URL=https://spire-spiffe-oidc-discovery-provider.spire-server.svc.cluster.local:443

          cat /data/token.svid
          JWT=$(cat /data/token.svid | jq -r '.[] | select(.svids) | .svids[0].svid' | xargs)
          KID=$(echo $JWT | base64 -d 2>/dev/null | jq -r '.kid')
          # Retrieve public key from JWK set, match kid from JWT to locate the correct one
          curl -k -s --fail-with-body "${URL}"/keys | jq '.keys[] | select(.kid == "'${KID}'")' > public.pem
          # Verify JWT with public pem
          echo $JWT | /data/step crypto jwt verify --key=public.pem --alg=RS256 --subtle
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
        runAsNonRoot: true
        seccompProfile:
          type: RuntimeDefault
      volumeMounts:
      - mountPath: /data
        name: data-volume
  restartPolicy: Never
  volumes:
  - csi:
      driver: csi.spiffe.io
      readOnly: true
    name: spire-api
  - name: data-volume
    emptyDir: {}
---
# Source: spire/charts/spire-server/templates/tests/test-connection.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "spire-server-test-connection"
  namespace: spire-server
  labels:
    helm.sh/chart: spire-server-0.1.0
    app.kubernetes.io/name: server
    app.kubernetes.io/instance: spire
    app.kubernetes.io/version: "1.9.6"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: test
  annotations:
    "helm.sh/hook": test
spec:
  securityContext:
    fsGroupChangePolicy: OnRootMismatch
  containers:
    - name: curl
      image: cgr.dev/chainguard/bash:latest@sha256:8c9e5cbb641ced8112c637eb3611dab29bf65448a9d884a03938baf1b352dc4d
      command: ['bash']
      args:
        - -c
        - |
          curl -f -s 'https://spire-server:443'
          NOCA=$?
          curl -k -f -s 'https://spire-server:443'
          IGNORECA=$?
          echo $NOCA $IGNORECA
          if [ $NOCA -eq 60 -a $IGNORECA -eq 22 ]; then
            # We were able to connect to the server but didn't recognize the ca (60) and the page not found (22) because we're not using grpc
            exit 0
          fi
          exit 1
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
        runAsNonRoot: true
        seccompProfile:
          type: RuntimeDefault
  restartPolicy: Never
---
# Source: spire/charts/spiffe-oidc-discovery-provider/templates/pre-delete-hook.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: spire-spiffe-oidc-discovery-provider-pre-delete
  namespace: spire-server
  labels:
    helm.sh/chart: spiffe-oidc-discovery-provider-0.1.0
    app.kubernetes.io/name: spiffe-oidc-discovery-provider
    app.kubernetes.io/instance: spire
    app.kubernetes.io/version: "1.9.6"
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": pre-delete
    "helm.sh/hook-delete-policy": before-hook-creation, hook-succeeded, hook-failed
spec:
  template:
    metadata:
      name: spire-spiffe-oidc-discovery-provider-pre-delete
    spec:
      restartPolicy: Never
      serviceAccountName: spire-spiffe-oidc-discovery-provider-pre-delete
      securityContext:
        fsGroupChangePolicy: OnRootMismatch
      containers:
      - name: pre-delete-job
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
          readOnlyRootFilesystem: true
          runAsNonRoot: true
          seccompProfile:
            type: RuntimeDefault
        image: docker.io/rancher/kubectl:v1.28.0
        args:
          - delete
          - -n
          - spire-server
          - deployment
          - spire-spiffe-oidc-discovery-provider
          - --wait
---
# Source: spire/charts/spire-server/templates/post-install-hook.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: spire-server-post-install
  namespace: spire-server
  labels:
    helm.sh/chart: spire-server-0.1.0
    app.kubernetes.io/name: server
    app.kubernetes.io/instance: spire
    app.kubernetes.io/version: "1.9.6"
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": post-install
    "helm.sh/hook-delete-policy": before-hook-creation, hook-succeeded, hook-failed
spec:
  template:
    metadata:
      name: spire-server-post-install
    spec:
      restartPolicy: Never
      serviceAccountName: spire-server-post-install
      securityContext:
        fsGroupChangePolicy: OnRootMismatch
      containers:
      - name: post-install-job
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
          readOnlyRootFilesystem: true
          runAsNonRoot: true
          seccompProfile:
            type: RuntimeDefault
        image: docker.io/rancher/kubectl:v1.28.0
        args:
          - patch
          - validatingwebhookconfiguration
          - spire-server-spire-controller-manager-webhook
          - --type=strategic
          - -p
          - |
            {
              "webhooks":[
                {
                  "name":"vclusterspiffeid.kb.io",
                  "failurePolicy":"Fail"
                },
                {
                  "name":"vclusterfederatedtrustdomain.kb.io",
                  "failurePolicy":"Fail"
                }
              ]
            }
---
# Source: spire/charts/spire-server/templates/post-upgrade-hook.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: spire-server-post-upgrade
  namespace: spire-server
  labels:
    helm.sh/chart: spire-server-0.1.0
    app.kubernetes.io/name: server
    app.kubernetes.io/instance: spire
    app.kubernetes.io/version: "1.9.6"
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": post-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation, hook-succeeded, hook-failed
spec:
  template:
    metadata:
      name: spire-server-post-upgrade
    spec:
      restartPolicy: Never
      serviceAccountName: spire-server-post-upgrade
      securityContext:
        fsGroupChangePolicy: OnRootMismatch
      containers:
      - name: post-upgrade-job
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
          readOnlyRootFilesystem: true
          runAsNonRoot: true
          seccompProfile:
            type: RuntimeDefault
        image: docker.io/rancher/kubectl:v1.28.0
        args:
          - patch
          - validatingwebhookconfiguration
          - spire-server-spire-controller-manager-webhook
          - --type=strategic
          - -p
          - |
            {
              "webhooks":[
                {
                  "name":"vclusterspiffeid.kb.io",
                  "failurePolicy":"Fail"
                },
                {
                  "name":"vclusterfederatedtrustdomain.kb.io",
                  "failurePolicy":"Fail"
                }
              ]
            }
---
# Source: spire/charts/spire-server/templates/pre-upgrade-hook.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: spire-server-pre-upgrade
  namespace: spire-server
  labels:
    helm.sh/chart: spire-server-0.1.0
    app.kubernetes.io/name: server
    app.kubernetes.io/instance: spire
    app.kubernetes.io/version: "1.9.6"
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": pre-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation, hook-succeeded, hook-failed
spec:
  template:
    metadata:
      name: spire-server-pre-upgrade
    spec:
      restartPolicy: Never
      serviceAccountName: spire-server-pre-upgrade
      securityContext:
        fsGroupChangePolicy: OnRootMismatch
      containers:
      - name: post-install-job
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
          readOnlyRootFilesystem: true
          runAsNonRoot: true
          seccompProfile:
            type: RuntimeDefault
        image: docker.io/rancher/kubectl:v1.28.0
        args:
          - patch
          - validatingwebhookconfiguration
          - spire-server-spire-controller-manager-webhook
          - --type=strategic
          - -p
          - |
            {
              "webhooks":[
                {
                  "name":"vclusterspiffeid.kb.io",
                  "failurePolicy":"Ignore"
                },
                {
                  "name":"vclusterfederatedtrustdomain.kb.io",
                  "failurePolicy":"Ignore"
                }
              ]
            }
