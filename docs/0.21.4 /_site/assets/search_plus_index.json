{"/docs/community/": {
    "title": "Community",
    "keywords": "",
    "url": "/docs/community/",
    "body": "edit this page on GitHub ✏️ Hi 👋 Thank you for wanting to be part of the community ❤️. This page outlines various community events and communication channels. Empower Our Journey ✊ ⭐️ Star VMware Secrets Manager to show your support. Your support helps us to reach out to even more people with this amazing tech. Join Our Community on Slack Open Source is better together. Join VMware Secret Manager’s Slack Workspace and let us change the world together 🤘. As a small, dedicated team of security enthusiasts, we value focused, effective communication. Thus, we prefer to consolidate our interactions into a single channel, rather than dispersing them across multiple platforms. Join Our Public Meetings We Are Stronger Together VMware Secrets Manager meetings are open for everyone, you are more than welcome to join. VSecM Contributor Sync Date/Time: Every last Thursday on every month @ 08:00am Pacific Calendar ICS Meetings Notes Zoom Meeting Goals Discuss VMware Secrets Manager project direction and roadmap. Provide a high-bandwidth forum in which the community can voice needs and make proposals. Achieve maintainer consensus on architectural decisions related to major VMware Secrets Manager features. Non-Goals VMware Secrets Manager maintenance. Triaging, troubleshooting, and resolving issues. Thank You ❤️ Thanks so much for your interest: It means a lot 🙏."
  },"/docs/contributor-sync/": {
    "title": "Contributor Hours",
    "keywords": "",
    "url": "/docs/contributor-sync/",
    "body": "edit this page on GitHub ✏️ Welcome 👋 Welcome to the VMware Secrets Manager Contributor Hour, a monthly meeting dedicated to the VMware Secrets Manager (VSecM) project. This meeting serves as a platform for contributors, maintainers, and anyone interested in the project to come together and discuss key aspects of VMware Secrets Manager’s future. Date/Time: Every last Thursday on every month @ 08:00am Pacific Calendar ICS Meetings Notes Zoom Meeting Goals Discuss VMware Secrets Manager project direction and roadmap. Provide a high-bandwidth forum in which the community can voice needs and make proposals. Achieve maintainer consensus on architectural decisions related to major VMware Secrets Manager features. Non-Goals VMware Secrets Manager maintenance. Triaging, troubleshooting, and resolving issues. Meeting Recordings VMware Secrets Manager Contributor Sync — 2023-09-29"
  },"/docs/contact/": {
    "title": "Contact",
    "keywords": "",
    "url": "/docs/contact/",
    "body": "edit this page on GitHub ✏️ Hi 👋 Thank you for wanting to contact us ❤️. This page outlines the various ways you can touch base. Ask Questions on Slack *VMware Secret Manager’s Slack Workspace** if one of the best ways to get in touch with the community, ask questions and get help. Create a GitHub Issue In addition to Slack, you can create an issue on GitHub to report a bug, request a feature, or ask a question. Also, please discuss your idea and findings with the community on VSecM’s Slack workspace whenever possible. Report a Security Vulnerability If you have found a security vulnerability, please follow this guideline to responsibly disclose it. Contribute to VMware Secrets Manager We appreciate your interest in making the world a better and more secure place where everyone can #sleepmore 🤗. We’d love to have you on board. Please read the contribution guidelines and the code of conduct, and let us know if you need anything else. Thank You ❤️ Thanks so much for your interest: It means a lot 🙏."
  },"/docs/maintainers/": {
    "title": "Maintainers",
    "keywords": "",
    "url": "/docs/maintainers/",
    "body": "edit this page on GitHub ✏️ Hi 👋 VMware Secrets Manager for Cloud-Native Apps is maintained by a group of dedicated individuals listed in the CODEOWNERS file on the VMware Secrets Manager GitHub repository. Lead Architect Volkan Özçelik @v0lkan As the lead architect, Volkan is responsible for overall code quality, security, architecture decisions, and major feature developments. Security Ramiro Salas @RamXX Ramiro is in charge of maintaining and enhancing the security aspects of the project. Core Contributors Abhishek Sharma @abhishek44sharma Abhishek is a core contributor, working on various aspects of the project. Esteban Serrano @st96d045 Esteban is a core contributor, focusing on integrating VSecM to various external components and hardening it for production use-cases. Arun Thundyill Saseendran @ats0stv Arun is a core contributor, assisting in feature development and workflow automation. Farhan Pasha @farhan-pasha Farhan is a core contributor, actively contributing to the codebase and assisting in feature development."
  },"/docs/security/": {
    "title": "Security",
    "keywords": "",
    "url": "/docs/security/",
    "body": "edit this page on GitHub ✏️ Secure All the Things 🛡️ This section contains how we handle security in VSecM. Vulnerability Management And Remediation Policy Ensuring the security of VMware Secrets Manager is a top priority and a continuous process that involves various proactive and reactive measures. On such measure is promptly addressing the security vulnerabilities that are found in the codebase. All medium and higher severity exploitable vulnerabilities discovered with dynamic code analysis MUST be fixed in a timely way after they are confirmed. This is mandatory to maintain the highest security standards for the project. For additional context, here is how we categorize the severity of the vulnerabilities: Medium Severity: A vulnerability that allows unauthorized disclosure of information or unauthorized modification of a system. High Severity: A vulnerability that allows unauthorized administrative access to a system. Target Response Times Our target time frame is 60 days for fixing vulnerabilities of medium or higher severity. That is to say, we do our best so that our codebase does not contain any public vulnerabilities of medium or higher severity that have been known for more than 60 days. In addition, our target initial response time for any vulnerability report received in the last 6 months is less than or equal to 14 days. About Keys and Nonce Values Cryptography plays a vital role in securing both data at rest and in transit. Ensuring that cryptographic keys and nonces are generated securely is fundamental to maintaining a robust security posture. WMware Secrets Manager generates all cryptographic keys and nonces that it needs using secure random generators. In the event that this practice is not followed, a bug MUST be filed to address this issue immediately. About Tests High-quality software is not just about features and performance; it’s also about reliability and predictability. Automated testing is a cornerstone in achieving these goals. Here is an informal guideline about how we approach testing in VMware Secrets Manager: As major new functionality is added to the software produced by this project, corresponding tests MUST be added to an automated test suite. In this context, “Major New Functionality” means features or changes that substantially alter the behavior, capabilities, or user experience of the software. Report a Security Vulnerability We take VMware Secrets Manager’s security seriously. If you believe you have found a vulnerability, please follow this guideline to responsibly disclose it."
  },"/docs/blog/": {
    "title": "VSecM Blog",
    "keywords": "",
    "url": "/docs/blog/",
    "body": "edit this page on GitHub ✏️ Keep Your Secrets… Secret-@v0lkan In the ever-evolving landscape of cloud-native applications, secrets management is critical to ensuring sensitive information’s security and integrity. While several solutions are… Read More"
  },"/docs/navigation/": {
    "title": "Preface",
    "keywords": "",
    "url": "/docs/navigation/",
    "body": "edit this page on GitHub ✏️ Hey, This Website Looks Like a GitBook 📖 Yes, this website is intentionally created like a book. We wanted to make sure that you have a great experience reading our documentation, and we believe that following the documentation cover-to-cover as if it were a book is the best way to learn about VMware Secrets Manager (VSecM) for Cloud-Native Apps. We use a heavily customized version of the Jekyll GitBook Theme to achieve this. You can check the source code for this website on GitHub to see how that is done. Terminology: A Tale of Two Secrets There are two kinds of “secret”s mentioned throughout this documentation: Secrets that are stored in VSecM Safe: When discussing these, they will be used like a regular word “secret” or, emphasized “secret”; however, you will never see them in monotype text. The other kind of secret is Kubernetes Secret objects. Those types will be explicitly mentioned as “Kubernetes Secrets” in the documentation. We hope this will clarify any confusion going forward."
  },"/docs/about/": {
    "title": "What is VSecM?",
    "keywords": "",
    "url": "/docs/about/",
    "body": "edit this page on GitHub ✏️ Why Do We Need a Cloud-Native Secrets Manager? Before we begin, let’s think about why we need a cloud-native secrets manager in the first place. Can’t we just use Kubernetes Secrets? The following section will answer this question. Wait, What’s Wrong With Kubernetes Secrets? Kubernetes Secrets have legitimate use cases; however, the out-of-the-box security provided by Kubernetes Secrets might not always meet the stringent security and flexibility demands of modern applications. In the Kubernetes ecosystem, the handling of secrets is facilitated through a specialized resource known as a Secret. The Secret resource allows Kubernetes to manage and store key-value pairs of sensitive data within a designated namespace in the cluster. Kubernetes Secrets can be widespread across the cluster into various namespaces which makes the management and access control to them tricky. In addition, when you update a Kubernetes Secret it is hard to make the workloads be aware of the change. Moreover, due to namespace isolation, you cannot define a cluster-wide or cross-cluster-federated secrets: You have to tie your secrets to a single namespace, which, at times, can be limiting. All of these (and more) is possible with VMware Secrets Manager. The VMware Secrets Manager Difference Cloud-native secret management can be more secure, centralized, and easy-to-use. This is where VMware Secrets Manager, comes into play: VMware Secrets Manager offers a secure, user-friendly, cloud-native secrets store that’s robust yet lightweight, requiring almost zero DevOps skills for installation and maintenance. In addition, VMware Secrets Manager… Has the ability to change secrets dynamically at runtime without having to reboot your workloads, Keeps encrypted backups of your secrets, Records last creation and last update timestamps for your secrets, Has a version history for your secrets, Stores backups of your secrets encrypted at rest, Enables GoLang transformations on your secrets, Can interpolate your stored secrets onto Kubernetes Secrets, Enables federation of your secrets across namespaces and clusters, and more. These are not achievable by using Kubernetes Secrets only. Where NOT to Use VMware Secrets Manager VMware Secrets Manager is not a Database, nor is it a distributed caching layer. Of course, you may tweak it to act like one if you try hard enough, yet, that is generally not a good use of the tool. VMware Secrets Manager is suitable for storing secrets and dispatching them; however, it is a terrible idea to use it as a centralized database to store everything but the kitchen sink. Use VMware Secrets Manager to store service keys, database credentials, access tokens, etc. How Do I Get the Root Token? Where Do I Store It? Unlike other “vault”-style secrets stores, VMware Secrets Manager requires no admin token for operation—a clear advantage that lets your Ops team #sleepmore due to automation and eliminates manual unlocking after system crashes. However, there’s no free lunch, and as the operator of a production system, your homework is to secure access to your cluster. Check out the Production Deployment Guidelines for further instructions about hardening your cluster to securely use VMware Secrets Manager. Still Want Your Root Tokens? Although VMware Secrets Manager does not require a root token, you can still provide one if you want to. Though, when you do that, you will have to manually unlock the system after a crash. If you let VMware Secrets Manager generate the root token for you, you will not have to worry about this, and when the system crashes, it will automatically unlock itself, so you can #sleepmore."
  },"/docs/endorsements/": {
    "title": "Things People Say",
    "keywords": "",
    "url": "/docs/endorsements/",
    "body": "edit this page on GitHub ✏️ Don’t just take our word for it. See what industry experts are saying about VMware Secrets Manager for Cloud-Native Apps. “This technology is the best I’ve seen to manage secrets end-to-end in a Kubernetes-native way. VMware Secrets Manager avoids the need to deploy and maintain external, non-cloud-native stores, which is a huge differentiator.” — Ramiro Salas, Staff Engineer, VMware “One more cloud native project that leverages SPIFFE for authentication! Woohoo! More SPIFFE!” — Eli Nesterov, Co-founder of SPIRL “I’ve long wondered how we could radically re-make secrets management if SPIFFE was already in place. Wonder no longer!” — Andrew Jessup, Director of Product Management, HPE Security Engineering “Secrets management is a relatively mature technology, however its role in the cloud native landscape has been dramatically evolving. VMware Secrets Manager is a ground-up re-imagination, solving the problem in a way that is congruent with the future of cloud computing.” — Evan Gilman, Author of Zero Trust Networks (O’Reilly), SPIRE core maintainer “With VMware Secrets Manager, dispatching my secrets in my infra is a breeze. It requires minimal effort to maintain, freeing me up to focus on more important tasks. In fact, it runs so smoothly that I sometimes forget it even exists. VMware Secrets Manager gives me peace of mind.” — Hamza Erbay, Staff Engineer, Udemy “VMware Secrets Manager is like a vault for your secrets, but instead of a big strong guy named Chuck guarding it, it is just some secure code.” — İlter Köse, Software Engineer, MOIA"
  },"/docs/quickstart/": {
    "title": "Quickstart",
    "keywords": "",
    "url": "/docs/quickstart/",
    "body": "edit this page on GitHub ✏️ Get Your Hands Dirty This is a quickstart guide to get you up and running with VSecM. Prerequisites Minikube: You can install VMware Secrets Manager on any Kubernetes cluster, but we’ll use Minikube in this quickstart example. Minikube is a tool that makes it easy to run Kubernetes locally. make: You’ll need make to run certain build tasks. You can install make using your favorite package manager. Docker: This quickstart guide assumes that Minikube uses the Docker driver. If you use a different driver, things will still likely work, but you might need to tweak some of the commands and configuration. I Have a Kubernetes Cluster Already If you are already have a cluster and a kubectl that you can use on that cluster, you won’t need Minikube, so you can skip the steps related to initializing Minikube and configuring Minikube-related environment variables. Also, if you are not using minikube, you will not need a local docker instance either. A Video Is Worth A Lot of Words Here’s a video that walks you through the steps in this quickstart guide: Clone the Repository Let’s start by cloning the VMware Secrets Manager repository first: cd $WORKSPACE git clone https://github.com/vmware-tanzu/secrets-manager.git cd secrets-manager Initialize Minikube Next, let’s initialize Minikube: cd $WORKSPACE/secrets-manager # If you have a previous minikube setup, you can delete it with: # make k8s-delete make k8s-start Configure Docker Environment Variables Next, let’s configure the Docker environment variables for Minikube. We don’t strictly need this for the quickstart, but it’s a good idea to do it anyway. eval $(minikube -p minikube docker-env) Install SPIRE and VMware Secrets Manager Next we’ll install SPIRE and VMware Secrets Manager on the cluster. cd $WORKSPACE/secrets-manager make deploy This will take a few minutes to complete. Verify Installation Let’s check that the installation was successful by listing the pods int the spire-system and vsecm-system namespaces: kubectl get po -n spire-system # Output: NAME READY STATUS RESTARTS spire-agent-p9m27 3/3 Running 1 (23s ago) spire-server-6fb4f57c8-6s7ns 2/2 Running 0 kubectl get po -n vsecm-system # Output: NAME READY STATUS RESTARTS vsecm-safe-85dd95949c-f4mhj 1/1 Running 0 vsecm-sentinel-6dc9b476f-djnq7 1/1 Running 0 All the pods look up and running, so we can move on to the next step. List Available Commands make help lists all the available make targets in a cheat sheet format: make help # The output will vary as we add more commands to the Makefile. # It will contain useful information about the available commands. Deploy an Example Workload Now, let’s deploy an example workload to the cluster to test VMware Secrets Manager in action. cd $WORKSPACE/secrets-manager make example-sdk-deploy This will take a few moments too. When done you would ba able to list the pods in the default namespace: kubectl get po -n default # Output NAME READY STATUS RESTARTS AGE example-68997489c6-8j8kj 1/1 Running 0 1m51s Let’s check the logs of our example workload: kubectl logs example-68997489c6-8j8kj The output will be something similar to this: 2023/07/28 01:26:51 fetch Failed to read the secrets file. Will retry in 5 seconds… Secret does not exist 2023/07/28 01:27:03 fetch Failed to read the secrets file. Will retry in 5 seconds… Secret does not exist 2023/07/28 01:27:08 fetch Failed to read the secrets file. Will retry in 5 seconds… … truncated … Our sample workload is trying to fetch a secret, but it can’t find it. Here’s the source code of our sample workload to provide some context: package main // … truncated headers … func main() { // … truncated irrelevant code … for { log.Println(\"fetch\") d, err := sentry.Fetch() if err != nil { fmt.Println(\"Failed. Will retry in 5 seconds…\") fmt.Println(err.Error()) time.Sleep(5 * time.Second) continue } if d.Data == \"\" { fmt.Println(\"No secret yet… will check again later.\") time.Sleep(5 * time.Second) continue } fmt.Printf( \"secret: updated: %s, created: %s, value: %s\\n\", d.Updated, d.Created, d.Data, ) time.Sleep(5 * time.Second) } } What the demo workload does is to try to fetch a secret every 5 seconds using the sentry.Fetch() function. sentry.Fetch() is a function provided by the VMware Secrets Manager; it establishes a secure mTLS connection between the workload and VSecM Safe to fetch the secret. Since this workload does not have any secret registered, the request fails and the workload retries every 5 seconds. Since this is a quickstart example, we won’t dive into the details of how the workload establishes a secure mTLS connection with the VSecM Safe. We’ll cover this in the following sections. For the sake of this quickstart, we can assume that secure communication between the workload and the VSecM Safe is already taken care of for us. Register a Secret Now, let’s register a secret and see what happens. To register a secret we’ll need to find the vsecm-sentinel pod in the vsecm-system namespace and execute a command inside the pod. Let’s get the pod first: kubectl get po -n vsecm-system Here’s a sample output: NAME READY STATUS RESTARTS vsecm-safe-85dd95949c-f4mhj 1/1 Running 0 vsecm-sentinel-6dc9b476f-djnq7 1/1 Running 0 vsecm-sentinel-6dc9b476f-djnq7 is what we need here. Let’s use it and register a secret to our example workload: kubectl exec vsecm-sentinel-6dc9b476f-djnq7 -n vsecm-system -- \\ safe -w \"example\" -n \"default\" -s \"VSecMRocks\" Sentinel Command Line Help VSecM Sentinel comes with a command line tool called safe. safe allows you to register secrets to VSecM Safe, delete secrets, or list existing secrets. You can execute safe -h or safe --help to get a list of available commands and options. You’ll get an OK as a response: OK For the command safe -w \"example\" -n \"default\" -s \"VSecMRocks\" -w is the workload name -n is the namespace -s is the secret value But how do you know what the workload name is? That’s where ClusterSPIFFEID comes in: kubectl get ClusterSPIFFEID And here’s the output: NAME AGE example 4h33m vsecm-safe 4h35m vsecm-sentinel 4h35m ClusterSPIFFEID with an Analogy Imagine the ClusterSPIFFEID as a badge maker for an organization. If anyone could create or modify badges (SVIDs), they could make one for themselves that mimics the CEO’s badge, gaining access to restricted areas. Hence, only trusted personnel (with elevated privileges) are allowed to manage the badge maker. Make sure your guard your ClusterSPIFFEID with proper RBAC rules. Let’s see the details of this example SPIFFE ID: kubectl describe ClusterSPIFFEID example And the output: Name: example Namespace: Labels: &lt;none&gt; Annotations: &lt;none&gt; API Version: spire.spiffe.io/v1alpha1 Kind: ClusterSPIFFEID Metadata: Creation Timestamp: 2023-07-28T01:26:48Z Generation: 1 Resource Version: 832 UID: b254294e-eed0-4116-8b38-2fb1d101e387 Spec: Pod Selector: Match Labels: app.kubernetes.io/name: example Spiffe ID Template: spiffe://vsecm.com/workload/example/ ns/{{ .PodMeta.Namespace }}/ sa/{{ .PodSpec.ServiceAccountName }}/n/{{ .PodMeta.Name }} Workload Selector Templates: k8s:ns:default k8s:sa:example Status: Stats: Entries Masked: 0 Entries To Set: 1 Entry Failures: 0 Namespaces Ignored: 4 Namespaces Selected: 6 Pod Entry Render Failures: 0 Pods Selected: 1 For the sake of keeping things simple because this is a quickstart, we can assume that someone has created this example SPIFFE ID for us, and using this SPIFFE ID, our example workload can securely communicate with the VSecM Safe. Verifying Secret Registration Since we’ve registered a secret, let’s see if our example workload can fetch the secret now and display it in its logs. kubectl logs example-68997489c6-8j8kj And the output would be something like this: 2023/07/28 06:06:39 fetch secret: updated: \"2023-07-28T01:34:30Z\", created: \"2023-07-28T01:34:30Z\", value: VSecMRocks 2023/07/28 06:06:44 fetch secret: updated: \"2023-07-28T01:34:30Z\", created: \"2023-07-28T01:34:30Z\", value: VSecMRocks 2023/07/28 06:06:49 fetch secret: updated: \"2023-07-28T01:34:30Z\", created: \"2023-07-28T01:34:30Z\", value: VSecMRocks As you can see, the secret is now fetched and displayed in the logs. The beauty of this approach is when we change the secret using VSecM Sentinel, the workload will automatically fetch the new value, without having to restart itself. Where to Go From Here This quickstart is meant to give you a quick overview of how you can use VMware Secrets Manager to securely manage secrets in your Kubernetes clusters. After successfully completing this quickstart, you can try the following: Join the VMware Secrets Manager Community on Slack where helpful community members and VMware Secrets Manager engineers hang out and answer questions. Navigate this website to learn more about VMware Secrets Manager, starting with its architecture, and design philosophy. Follow a more detailed tutorial that contains multiple use cases."
  },"/docs/philosophy/": {
    "title": "VSecM Design Philosophy",
    "keywords": "",
    "url": "/docs/philosophy/",
    "body": "edit this page on GitHub ✏️ Introduction We follow the guidelines outlined in the next few sections as an architectural baseline. Do One Thing Well At a 5000-feet level, VMware Secrets Manager is a secure Key-Value store. It can securely store arbitrary values that you, as an administrator, associate with keys. It does that, and it does that well. If you are searching for a solution to create and store X.509 certificates, create dynamic secrets, automate your PKI infrastructure, federate your identities, use as an OTP generator, policy manager, in short, anything other than a secure key-value store, then VMware Secrets Manager is likely not the solution you are looking for. Be Kubernetes-Native VMware Secrets Manager is designed to run on Kubernetes and only on Kubernetes. That helps us leverage Kubernetes concepts like Operators, Custom Resources, and Controllers to our advantage to simplify workflow and state management. If you are looking for a solution that runs outside Kubernetes or as a standalone binary, then VMware Secrets Manager is not the Secrets Store you’re looking for. Have a Minimal and Intuitive API As an administrator, there is a limited set of API endpoints that you can interact with VMware Secrets Manager. This makes VMware Secrets Manager easy to manage. In addition, a minimal set of APIs means a smaller attack surface, a smaller footprint, and a codebase that is easy to understand, test, audit, and develop; all good things. Be Practically Secure Corollary: Do not be annoyingly secure. Provide a delightful user experience while taking security seriously. VMware Secrets Manager is a secure solution, yet still delightful to operate. You won’t have to jump over the hoops or wake up in the middle of the night to keep it up and running. Instead, VMware Secrets Manager will work seamlessly, as if it doesn’t exist at all. Secure By Default VMware Secrets Manager stores your sensitive data in memory. None of your secrets are stored as plain text on disk. Anything that VMware Secrets Manager saves to disk is encrypted. Yes, that brings up resource limitations since you cannot store a gorilla holding a banana and the entire jungle in your store; however, a couple of gigabytes of RAM can store a lot of plain text secrets, so it’s good enough for most practical purposes. More importantly, almost all modern instruction set architectures and operating systems implement memory protection. The primary purpose of memory protection is to prevent a process from accessing memory that has not been allocated to it. This prevents a bug or malware within a process from affecting other processes or the operating system itself. Therefore, reading a variable’s value from a process’s memory is practically impossible unless you attach a debugger to it. And that makes keeping plain text secrets in memory (and nowhere else than memory) crucial. For disaster recovery, VMware Secrets Manager (by default) backs up encrypted version of its state on the file system; however, the plain text secrets that VSecM Safe dispatches to workloads will always be stored in memory. Resilient By Default When an VMware Secrets Manager component crashes or when an VMware Secrets Manager component is evicted, the workloads can still function with the existing secrets they have without having to rely on the existence of an active secrets store. When an VMware Secrets Manager component restarts, it seamlessly recovers its state from an encrypted backup without requiring manual intervention. Conclusion While VMware Secrets Manager might not have all the bells and whistles of a full-blown security suite that integrates everything but the kitchen sink, we are sure that by following these guiding principles, it will remain a delightfully secure secrets manager that a lot of people will love to use."
  },"/docs/architecture/": {
    "title": "VSecM Architecture",
    "keywords": "",
    "url": "/docs/architecture/",
    "body": "edit this page on GitHub ✏️ Introduction This section discusses VMware Secrets Manager architecture and building blocks in greater detail: We will cover VMware Secrets Manager’s system design and project structure. You don’t have to know about these architectural details to use VMware Secrets Manager; however, understanding how VMware Secrets Manager works as a system can prove helpful when you want to extend, augment, or optimize VMware Secrets Manager. Also, if you want to contribute to the VMware Secrets Manager source code, knowing what happens under the hood will serve you well. Components of VMware Secrets Manager VMware Secrets Manager (VSecM), as a system, has the following components. SPIRE SPIRE is not strictly a part of VMware Secrets Manager. However, VMware Secrets Manager uses SPIRE to establish an Identity Control Plane. SPIRE is what makes communication within VMware Secrets Manager components and workloads possible. It dispatches x.509 SVID Certificates to the required parties to make secure mTLS communication possible. Check out the official SPIFFE/SPIRE documentation for more information about how SPIRE works internally. It Is More SPIFFE than SPIRE Technically, any SPIFFE-compatible Identity Control Plane can be used with VMware Secrets Manager. However, only SPIRE comes as part of the default installation. VSecM Safe vsecm-safe stores secrets and dispatches them to workloads. VSecM Sidecar vsecm-sidecar is a sidecar that facilitates delivering secrets to workloads. VSecM Sentinel vsecm-sentinel is a pod you can shell in and do administrative tasks such as registering secrets for workloads. Here is a simplified overview of how various actors on a VMware Secrets Manager system interact with each other: High-Level Architecture Dispatching Identities SPIRE delivers short-lived X.509 SVIDs to VMware Secrets Manager components and consumer workloads. VSecM Sidecar periodically talks to VSecM Safe to check if there is a new secret to be updated. Open the image above in a new tab or window to see it in full size. Creating Secrets VSecM Sentinel is the only place where secrets can be created and registered to VSecM Safe. Component and Workload SVID Schemas SPIFFE ID format wor workloads is as follows: spiffe://vsecm.com/workload/$workloadName /ns/{{ .PodMeta.Namespace }} /sa/{{ .PodSpec.ServiceAccountName }} /n/{{ .PodMeta.Name }} For the non-vsecm-system workloads that Safe injects secrets, $workloadName is determined by the workload’s ClusterSPIFFEID CRD. For vsecm-system components, we use vsecm-safe and vsecm-sentinel for the $workloadName (along with other attestors such as attesting the service account and namespace): spiffe://vsecm.com/workload/vsecm-safe /ns/{{ .PodMeta.Namespace }} /sa/{{ .PodSpec.ServiceAccountName }} /n/{{ .PodMeta.Name }} spiffe://vsecm.com/workload/vsecm-sentinel /ns/{{ .PodMeta.Namespace }} /sa/{{ .PodSpec.ServiceAccountName }} /n/{{ .PodMeta.Name }} Persisting Secrets VSecM Safe uses age to securely persist the secrets to disk so that when its Pod is replaced by another pod for any reason (eviction, crash, system restart, etc.). When that happens, VSecM Safe can retrieve secrets from a persistent storage. Age Algorithm Can We Swapped With SHA-256 The age algorithm can be swapped with SHA-256. Check out the VSecM Configuration for more information. Since decryption is relatively expensive, once a secret is retrieved, it is kept in memory and served from memory for better performance. Unfortunately, this also means the amount of secrets you have for all your workloads has to fit in the memory you allocate to VSecM Safe. VSecM Safe Bootstrapping Flow To persist secrets, VSecM Safe needs a way to generate and securely store the private and public age keys that are utilized for decrypting and encrypting the secrets, respectively. Key generation is conveniently provided by age Go SDK. After generation, the keys are stored in a Kubernetes Secret that only VSecM Safe can access. Here is a sequence diagram of the VSecM Safe bootstrapping flow: Note that, until bootstrapping is complete, VSecM Safe will not respond to any API requests that you make from VSecM Sentinel. You Can Swap the Encryption Mechanism In FIPS-compliant environments, you can use VMware Secrets Manager FIPS-compliant container images that use AES-256-GCM encryption instead of Age. Check out the Configuration section for more details. Also, you can opt-out from auto-generating the private and public keys and provide your own keys. However, when you do this, you will have to manually unlock VSecM Safe by providing your keys every time it crashes. If you let VSecM Safe auto-generate the keys, you won’t have to do this; so you can #sleepmore. Again, check out the Configuration section for more details. VSecM Safe Pod Layout Here is what an VSecM Safe Pod looks like at a high level: spire-agent-socket: Is a SPIFFE CSI Driver-managed volume that enables SPIRE to distribute X.509 SVIDs to the Pod. /data is the volume where secrets are stored in an encrypted format. You are strongly encouraged to use a persistent volume for production setups to retrieve the secrets if the Pod crashes and restarts. /key is where the secret vsecm-safe-age-key mounts. For security reasons, ensure that only the pod VSecM Safe can read and write to vsecm-safe-age-key and no one else has access. In this diagram, this is achieved by assigning a secret-readwriter role to VSecM Safe and using that role to update the secret. Any pod that does not have the role will be unable to read or write to this secret. If the main container does not have a public/private key pair in memory, it will attempt to retrieve it from the /key volume. If that fails, it will generate a brand new key pair and then store it in the vsecm-safe-age-key secret. Template Transformation and K8S Secret Generation Here is a sequence diagram of how the and VSecM Safe-managed secret is transformed into a Kubernetes Secret (open the image in a new tab for a larger version): There are two parts to this: Transforming secrets using a Go template transformation Updating the relevant Kubernetes Secret You can check VSecM Sentinel CLI Documentation for various ways this transformation can be done. In addition, you can check VSecM Secret Registration Tutorial for more information about how the Kubernetes Secret object is generated and used in workloads. Liveness and Readiness Probes VSecM Safe and VSecM Sentinel use liveness and readiness probes. These probes are tiny web servers that serve at ports 8081 and 8082 by default, respectively. You can set VSECM_PROBE_LIVENESS_PORT (default :8081) and VSECM_PROBE_READINESS_PORT (default :8082) environment variables to change the ports used for these probes. When the service is healthy, the liveness probe will return an HTTP 200 success response. When the service is ready to receive traffic, the readiness probe will return an HTTP 200 success response. Conclusion This was a deeper overview of VMware Secrets Manager architecture. If you have further questions, feel free to join the VMware Secrets Manager community on Slack and ask them out."
  },"/docs/installation/": {
    "title": "VSecM Installation",
    "keywords": "",
    "url": "/docs/installation/",
    "body": "edit this page on GitHub ✏️ Introduction There are several ways to install VMware Secrets Manager to a Kubernetes cluster: You can use helm charts, Or you can use the Makefile targets This page covers both approaches. Prerequisites Before you start, make sure you have the following prerequisites: You have helm installed on your system. You have kubectl installed on your system. You have a Kubernetes cluster running and kubectl is configured to connect to it. You have make installed on your system. Installing Using helm helm is the easiest way to install VMware Secrets Manager to your Kubernetes cluster. Make sure you have helm v3 installed and execute the following commands: helm repo add vsecm https://vmware-tanzu.github.io/secrets-manager/ helm repo update helm install vsecm vsecm/vsecm For detailed instruction on VMware Secrets Manager installation through Helm Charts please refer to VSecM Helm Charts README.md Installing Using make Make sure you have make and git installed in your system. First, clone the repository: cd $WORKSPACE git clone https://github.com/vmware-tanzu/secrets-manager.git cd secrets-manager Then, run the following command to install VMware Secrets Manager to your cluster: make deploy That’s it. You are all set 🤘. Verifying the Installation To verify installation, check out the vsecm-system and `spire-system namespaces: kubectl get po -n vsecm-system You should see something similar to the following output: NAME READY STATUS vsecm-safe-85dd95949c-f4mhj 1/1 Running vsecm-sentinel-6dc9b476f-djnq7 1/1 Running Then, do the same for spire-system namespace: kubectl get po -n spire-system You should see something similar to the following output: NAME READY STATUS spire-agent-p9m27 3/3 Running spire-server-6fb4f57c8-6s7ns 2/2 Running SPIRE Agent and Server Might Restart It is okay if you see the SPIRE Agent and Server pods restarting once or twice. They will eventually stabilize within a few moments. Uninstalling VMware Secrets Manager Uninstallation can be done by running a script: cd $WORKSPACE/secrets-manager ./hack/uninstall.sh Or, if you have installed VMware Secrets Manager using helm, you can use helm uninstall command: helm uninstall vsecm"
  },"/docs/cli/": {
    "title": "VSecM CLI",
    "keywords": "",
    "url": "/docs/cli/",
    "body": "edit this page on GitHub ✏️ Introduction This section contains usage examples and documentation for VSecM Sentinel’s Command Line Interface (CLI). Finding VSecM Sentinel First, find which pod belongs to vsecm-system: kubetctl get po -n vsecm-system The response to the above command will be similar to the following: NAME READY vsecm-safe-5f6948c84c-vkrdh 1/1 vsecm-sentinel-5998b5dbfc-lvw44 1/1 There, vsecm-sentinel-5998b5dbfc-lvw44 is the name of the Pod you’d need. You can also execute a script similar to the following to save the Pod’s name into an environment variable: SENTINEL=$(kubectl get po -n vsecm-system \\ | grep \"vsecm-sentinel-\" | awk '{print $1}') In the following examples, we’ll use $SENTINEL in lieu of the VSecM Sentinel’s Pod name. Displaying Help Information VSecM Sentinel has a binary called safe that can be used to interact with VSecM Safe API. You can use the following command to display help information for safe: kubectl exec $SENTINEL -n vsecm-system -- safe --help About --help The output of safe --help will depend on the version of safe you use; however, it will contain useful information about how to use the program. Registering a Secret for a Workload Given our workload has the SPIFFE ID \"spiffe://vsecm.com/workload/billing/: …[truncated]\" kubectl exec $SENTINEL -n vsecm-system -- safe \\ -w billing \\ -s \"very secret value\" will register the secret \"very secret value\" to billing. Registering Multiple Secrets You can use the -a (append) argument to register more than one secret to a workload. kubectl exec $SENTINEL -n vsecm-system -- safe \\ -w billing \\ -s \"first part of the token\" \\ -a kubectl exec $SENTINEL -n vsecm-system -- safe \\ -w billing \\ -s \"second part of the token\" \\ -a Encrypting a Secret Use the -e flag to encrypt a secret. kubectl exec $SENTINEL -n vsecm-system -- safe \\ -s \"very secret value\" \\ -e # The above command outputs an encrypted string that can be # securely stored anywhere, including source control systems. Registering an Encrypted Secret Again, -e flag can be used to register an encrypted secret to a workload: kubectl exec $SENTINEL -n vsecm-system -- safe \\ -w billing \\ -s $ENCRYPTED_SECRET \\ -e Deleting a Secret To delete the secret associated to a workload, use the -d flag: kubectl exec $SENTINEL -n vsecm-system -- safe \\ -w billing \\ -d Choosing a Backing Store The registered secrets will be encrypted and backed up to VSecM Safe’s Kubernetes volume by default. This behavior can be configured by changing the VSECM_SAFE_BACKING_STORE environment variable that VSecM Safe sees. In addition, this behavior can be overridden on a per-secret basis too. The following commands stores the secret to the backing volume (default behavior): kubectl exec $SENTINEL -n vsecm-system -- safe \\ -w billing \\ -s \"very secret value\" \\ -b file This one, will not store the secret on file; the secret will only be persisted in memory, and will be lost if VSecM Sentinel needs to restart: kubectl exec $SENTINEL -n vsecm-system -- safe \\ -w billing \\ -s \"very secret value\" \\ -b memory The following will stored the secret on the cluster itself as a Kubernetes Secret. The value of the secret will be encrypted with the public key of VSecM Safe before storing it on the Secret. kubectl exec $SENTINEL -n vsecm-system -- safe \\ -w billing \\ -s \"very secret value\" \\ -b cluster Template Transformations You can transform how the stored secret is displayed to the consuming workloads: kubectl exec \"$SENTINEL\" -n vsecm-system -- safe \\ -w \"example\" \\ -s '{\"username\": \"root\", \\ \"password\": \"SuperSecret\", \\ \"value\": \"VSecMRocks\"}' \\ -t '{\"USER\":\"{{.username}}\", \\ \"PASS\":\"{{.password}}\", \\ \"VALUE\": \"{{.value}}\"}' When the workload fetches the secret through the workload API, this is what it sees as the value: {\"USER\": \"root\", \"PASS\": \"SuperSecret\", \"VALUE\": \"VSecMRocks\"} Instead of this default transformation, you can output it as yaml too: kubectl exec \"$SENTINEL\" -n vsecm-system -- safe \\ -w \"example\" \\ -s '{\"username\": \"root\", \\ \"password\": \"SuperSecret\", \\ \"value\": \"VSecMRocks\"}' \\ -t '{\"USER\":\"{{.username}}\", \\ \"PASS\":\"{{.password}}\", \\ \"VALUE\": \"{{.value}}\"}' \\ -f yaml The above command will result in the following secret value to the workload that receives it: USER: root PASS: SuperSecret VALUE: VSecMRocks \"json\" Is the Default Value If you don’t specify the -f flag, it will default to \"json\". You can create a YAML secret value without the -t flag too. In that case VSecM Safe will assume an identity transformation: kubectl exec \"$SENTINEL\" -n vsecm-system -- safe \\ -w \"example\" \\ -s '{\"username\": \"root\", \\ \"password\": \"SuperSecret\", \\ \"value\": \"VSecMRocks\"}' \\ -f yaml The above command will result in the following secret value to the workload that receives it: username: root password: SuperSecret value: VSecMRocks If you provide -f json as the format argument, the secret will have to be a strict JSON. Otherwise, VSecM Sentinel will try to come up with a reasonable value, and not raise an error; however the output will likely be in a format that the workload is not expecting. Gotcha If -t is given, the -s argument will have to be a valid JSON regardless of what is chosen for -f. The following is also possible: kubectl exec \"$SENTINEL\" -n vsecm-system -- safe \\ -w \"example\" \\ -s 'USER»»»{{.username}}' and will result in the following as the secret value for the workload: USER»»»root Or, equivalently: kubectl exec \"$SENTINEL\" -n vsecm-system -- safe \\ -w \"example\" \\ -s 'USER»»»{{.username}}' \\ -f json Will provide the following to the workload: USER»»»root Similarly, the following will not raise an error: kubectl exec \"$SENTINEL\" -n vsecm-system -- safe \\ -w \"example\" \\ -s 'USER»»»{{.username}}' \\ -f yaml To transform the value to YAML, or JSON, -s has to be a valid JSON. Creating Kubernetes Secrets VSecM Safe-managed secrets can be interpolated onto Kubernetes secrets if a template transformation is given. Let’s take the following as an example: kubectl exec \"$SENTINEL\" -n vsecm-system -- safe \\ -w \"billing\" \\ -n \"finance\" \\ -s '{\"username\": \"root\", \"password\": \"SuperSecret\"}' \\ -t '{\"USERNAME\":\"{{.username}}\", \"PASSWORD\":\"{{.password}}\"' \\ -k The -k flag hints VSecM Safe that the secret will be synced with a Kubernetes Secret. -n tells that the namespace of the secret is \"finance\". Before running this command, a secret with the following manifest should exist in the cluster: apiVersion: v1 kind: Secret metadata: name: vsecm-secret-billing namespace: finance type: Opaque The vsecm-secret- prefix is required for VSecM Safe to locate the Secret. Also metadata.namespace attribute should match the namespace that’s provided with the -n flag to VSecM Sentinel. After executing the command the secret will contain the new values in its data: section. kubectl describe secret vsecm-secret-billing -n finance # Output: # Name: vsecm-secret-billing # Namespace: finance # Labels: &lt;none&gt; # Annotations: &lt;none&gt; # # Type: Opaque # # Data # ==== # USERNAME: 137 bytes # PASSWORD: 196 bytes Setting the Master Secret Manually VSecM Safe uses a master secret to encrypt the secrets that are stored. Typically, this master secret is stored as a Kubernetes secret for your convenience. However, if you want you can set VSECM_MANUAL_KEY_INPUT to \"true\" and provide the master secret manually. Although this approach enhances security, it also means that you will have to provide the master secret to VSecM Safe whenever the pod is evicted, or crashes, or restarts for any reason. Since, this brings a mild operational inconvenience, it is not enabled by default. kubectl exec \"$SENTINEL\" -n vsecm-system -- vsecm \\ --input-keys \"AGE-SECRET-KEY-1RZU…\\nage1…\\na6…ceec\" # Output: # # OK"
  },"/docs/sdk/": {
    "title": "VSecM SDK",
    "keywords": "",
    "url": "/docs/sdk/",
    "body": "edit this page on GitHub ✏️ SDK This is the documentation for VMware Secrets Manager Go SDK. Package sentry The current SDK has two public methods under the package sentry: func Fetch func Watch func Fetch() (string, error) Fetch fetches the up-to-date secret that has been registered to the workload. secret, err := sentry.Fetch() In case of a problem, Fetch will return an empty string and an error explaining what went wrong. func Watch() Watch synchronizes the internal state of the workload by talking to VSecM Safe regularly. It periodically calls Fetch() behind the scenes to get its work done. Once it fetches the secrets, it saves them to the location defined in the VSECM_SIDECAR_SECRETS_PATH environment variable (/opt/vsecm/secrets.json by default). Usage Example Here is a demo workload that uses the Fetch() API to retrieve secrets from VSecM Safe. package main import ( \"fmt\" \"github.com/vmware-tanzu/secrets-manager/sdk/sentry\" \"time\" ) func main() { for { // Fetch the secret bound to this workload // using VMware Secrets Manager Go SDK: data, err := sentry.Fetch() if err != nil { fmt.Println(\"Failed. Will retry…\") } else { fmt.Println(\"secret: '\", data, \"'\") } time.Sleep(5 * time.Second) } } Here follows a possible Deployment descriptor for such a workload. Check out VMware Secrets Manager demo workload manifests for additional examples. apiVersion: v1 kind: ServiceAccount metadata: name: example namespace: default automountServiceAccountToken: false --- apiVersion: spire.spiffe.io/v1alpha1 kind: ClusterSPIFFEID metadata: name: example spec: spiffeIDTemplate: \"spiffe://vsecm.com/workload/example\" podSelector: matchLabels: app.kubernetes.io/name: example --- apiVersion: apps/v1 kind: Deployment metadata: name: example namespace: default labels: app.kubernetes.io/name: example spec: replicas: 1 selector: matchLabels: app.kubernetes.io/name: example template: metadata: labels: app.kubernetes.io/name: example spec: serviceAccountName: example containers: - name: main image: vsecm/example-using-sdk:latest volumeMounts: - name: spire-agent-socket mountPath: /spire-agent-socket readOnly: true env: - name: SPIFFE_ENDPOINT_SOCKET value: unix:///spire-agent-socket/agent.sock volumes: - name: spire-agent-socket hostPath: path: /run/spire/sockets type: Directory You can also check out the relevant sections of the Registering Secrets article for an example of VMware Secrets Manager Go SDK usage."
  },"/docs/configuration/": {
    "title": "Configuring VSecM",
    "keywords": "",
    "url": "/docs/configuration/",
    "body": "edit this page on GitHub ✏️ Introduction VMware Secrets Manager system components can be configured using environment variables. The following section contain a breakdown of all of these environment variables. Looking for VMware Secrets Manager Production Tips? For production setup, check out VMware Secrets Manager Production Deployment. Environment Variables Using VSecM Helm Charts? If you are using VMware Secrets Manager Helm Charts, you can configure these environment variables using the values.yaml file. SPIFFE_ENDPOINT_SOCKET SPIFFE_ENDPOINT_SOCKET is required for VSecM Sentinel to talk to SPIRE. If not provided, a default value of \"unix:///spire-agent-socket/agent.sock\" will be used. VSECM_LOG_LEVEL VSECM_LOG_LEVEL determines the verbosity of the logs in VSecM Safe. VSecM Sidecar also uses this configuration; however, unlike VSecM Safe, it is not dynamic. While you can dynamically configure this at runtime for VSecM Safe without having to restart VSecM Safe, for VSecM Sidecar you’ll have to restart the workload’s pod for any changes to take effect. 0: logs are off, 7: highest verbosity. default: 3 Here are what various log levels correspond to: Off = 0 Fatal = 1 Error = 2 Warn = 3 Info = 4 Audit = 5 Debug = 6 Trace = 7 VSECM_WORKLOAD_SVID_PREFIX Both VSecM Safe and workloads use this environment variable. VSECM_WORKLOAD_SVID_PREFIX is required for validation. If not provided, it will default to: \"spiffe://vsecm.com/workload/\" VSECM_SENTINEL_SVID_PREFIX Both VSecM Safe and VSecM Sentinel use this environment variable. VSECM_SENTINEL_SVID_PREFIX is required for validation. If not provided, it will default to: \"spiffe://vsecm.com/workload/vsecm-sentinel/ns/vsecm-system/sa/vsecm-sentinel/n/\" VSECM_SAFE_FIPS_COMPLIANT VSECM_SAFE_FIPS_COMPLIANT is required for VSecM Safe to run in FIPS-compliant mode. Defaults to \"false\", which means VSecM Safe will run in non-FIPS-compliant mode. Setting it to \"true\" will make VSecM Safe run in FIPS-compliant mode. You Need Host Support for FIPS-Compliant Mode Note that this is not a guarantee that VSecM Safe will actually run in FIPS compliant mode, as it depends on the underlying base image. In addition, the host environment will need to be compliant too. If you are using one of the official FIPS-complaint VSecM Docker images, then it will be FIPS-compliant. As a FIPS-compliant base image you can choose from the following: vsecm/vsecm-ist-fips-safe (using a Distroless base) vsecm/vsecm-photon-fips-safe (using VMware Photon OS as a base) VSECM_SAFE_SVID_PREFIX Both VSecM Sentinel, VSecM Safe, and workloads use this environment variable. VSECM_SAFE_SVID_PREFIX is required for validation. If not provided, it will default to: \"spiffe://vsecm.com/workload/vsecm-safe/ns/vsecm-system/sa/vsecm-safe/n/\" VSECM_SAFE_DATA_PATH VSECM_SAFE_DATA_PATH is where VSecM Safe stores its encrypted secrets. If not given, defaults to \"/data\". VSECM_CRYPTO_KEY_PATH VSECM_CRYPTO_KEY_PATH is where VSecM Safe will fetch the \"key.txt\" that contains the encryption keys. If not given, it will default to \"/key/key.txt\". VSECM_CRYPTO_KEY_NAME VSECM_CRYPTO_KEY_NAME is how the age secret key is referenced by name inside VSecM Safe’s code. If not set, defaults to \"vsecm-safe-age-key\". If you change the value of this environment variable, make sure to change the relevant Secret and Deployment YAML manifests too. The easiest way to do this is to do a project wide search and find and replace places where reference \"vsecm-safe-age-key\" to your new name of choice. VSECM_MANUAL_KEY_INPUT VSECM_MANUAL_KEY_INPUT is used to tell VSecM Safe to bypass generating the master crypto key and instead use the key provided by the operator using VSecM Sentinel. Defaults to \"false\". When set to \"true\", VSecM Safe will not store the master key in a Kubernetes Secret; the master key will reside solely in the memory of VSecM Safe. The control offered by this approach regarding the threat boundary of VSecM Safe provides enhanced security compared to the default behavior where the master key is randomly-generated in a cryptographically secure way and stored in a Kubernetes Secret (which is still pretty secure, especially if you encrypt your etcd and establish a tight RBAC over the Kubernetes Secret that stores the master key). That being said, in manual input mode, all the cryptographic material will be kept in memory, and no Kubernetes Secrets will be harmed (which is even more secure). However, this approach does place the onus of securing the master key on you. Yet, don’t worry, storing the master key can be easily handled depending on your infrastructure: With the right setup, managing the master key is just another cog in the machinery of your security measures, not an additional burden. That’s why, although it’s not enabled by default, adopting this measure could be an additional step in bolstering your system’s security. Also note that when this variable is set to \"true\", VSecM Safe will not respond to API requests until a master key is provided, using VSecM Sentinel. VSECM_SAFE_SECRET_NAME_PREFIX VSECM_SAFE_SECRET_NAME_PREFIX is the prefix that is used to prepend to the secret names that VSecM Safe stores in the cluster as Secret objects when the -k option in VSecM Sentinel is selected. If this variable is not set or is empty, the default value \"vsecm-secret-\" is used. VSECM_SAFE_ENDPOINT_URL VSECM_SAFE_ENDPOINT_URL is the REST API endpoint that VSecM Safe exposes from its Service. VSecM Sentinel, VSecM Sidecar and workloads need this URL configured. If not provided, it will default to: \"https://vsecm-safe.vsecm-system.svc.cluster.local:8443/\". VSECM_PROBE_LIVENESS_PORT VSecM Safe and VSecM Sentinel use this configuration. VSECM_PROBE_LIVENESS_PORT is the port where the liveness probe will serve. Defaults to :8081. VSECM_PROBE_READINESS_PORT VSecM Safe uses this configuration. VSECM_PROBE_READINESS_PORT is the port where the readiness probe will serve. Defaults to :8082. VSECM_SAFE_BOOTSTRAP_TIMEOUT VSecM Safe uses this configuration. VSECM_SAFE_BOOTSTRAP_TIMEOUT is how long (in milliseconds) VSecM Safe will wait for an SPIRE X.509 SVID bundle before giving up and crashing. The default value is 30000 milliseconds. VSECM_SAFE_SOURCE_ACQUISITION_TIMEOUT VSECM_SAFE_SOURCE_ACQUISITION_TIMEOUT is the timeout duration for acquiring a SPIFFE source bundle. If the environment variable is not set, or cannot be parsed, defaults to 10000 milliseconds. VSECM_SAFE_IV_INITIALIZATION_INTERVAL VSECM_SAFE_IV_INITIALIZATION_INTERVAL is used as a security measure to time-based attacks where too frequent call of a function can be used to generate less-randomized AES IV values. If the environment variable is not set or contains an invalid integer, it defaults to 50 milliseconds. The value in the environment variable is in milliseconds. VSECM_SAFE_TLS_PORT VSECM_SAFE_TLS_PORT is the port that VSecM Safe serves its API endpoints. When you change this port, you will likely need to make changes in more than one manifest, and restart or redeploy VMware Secrets Manager and SPIRE. Defaults to \":8443\". VSECM_SAFE_SECRET_BUFFER_SIZE VSECM_SAFE_SECRET_BUFFER_SIZE is the amount of secret insertion operations to be buffered until VSecM Safe API blocks and waits for the buffer to have an empty slot. If the environment variable is not set, this buffer size defaults to 10. Two separate buffers of the same size are used for IO operations, and Kubernetes Secret creation (depending on the type of the API request). The Kubernetes Secrets buffer, and File IO buffer work asynchronously and independent of each other int two separate goroutines. VSECM_SAFE_K8S_SECRET_BUFFER_SIZE VSECM_SAFE_K8S_SECRET_BUFFER_SIZE is the buffer size for the VSecM Safe Kubernetes secret queue. If the environment variable is not set, the default buffer size is 10. VSECM_SAFE_SECRET_DELETE_BUFFER_SIZE VSECM_SAFE_SECRET_DELETE_BUFFER_SIZE isd the buffer size for the VSecM Safe secret deletion queue. If the environment variable is not set, the default buffer size is 10. VSECM_SAFE_K8S_SECRET_DELETE_BUFFER_SIZE VSECM_SAFE_K8S_SECRET_DELETE_BUFFER_SIZE the buffer size for the VSecM Safe Kubernetes secret deletion queue. If the environment variable is not set, the default buffer size is 10. VSECM_SAFE_BACKING_STORE This environment variable is used by VSecM Sentinel to let VSecM Safe know where to persist the secret. To reiterate, this environment variable shall be defined for VSecM Sentinel deployment; defining it for VSecM Safe has no effect. VSECM_SAFE_BACKING_STORE is the type of the storage where the secrets will be encrypted and persisted. If not given, defaults to \"file\". The other option is \"in-memory\". A \"file\" backing store means VSecM Safe persists an encrypted version of its state in a volume (ideally a PersistedVolume). An \"in-memory\" backing store means VSecM Safe does not persist backups of the secrets it created to disk. When that option is selected, you will lose all of your secrets if VSecM Safe is evicted by the scheduler or manually restarted by an operator. VSECM_SAFE_SECRET_BACKUP_COUNT VSECM_SAFE_SECRET_BACKUP_COUNT indicates the number of backups to keep for VSecM Safe secrets. If the environment variable VSECM_SAFE_SECRET_BACKUP_COUNT is not set or is not a valid integer, the default value of \"3\" will be used. This configuration is not effective when VSECM_SAFE_BACKING_STORE is set to \"in-memory\". VSECM_SAFE_USE_KUBERNETES_SECRETS VSECM_SAFE_USE_KUBERNETES_SECRETS is a flag indicating whether to create a plain text Kubernetes secret for the workloads registered. If the environment variable is not set or its value is not \"true\", it will be assumed \"false\". There are two things to note about this approach: First, by design, and for security reasons, the original Kubernetes Secret should exist, and it should be initiated to a default data as follows before it can be synced by VSecM Safe: apiVersion: v1 kind: Secret metadata: # The string after `vsecm-secret-` must match the # workload’s name. # For example, this is an VSecM-managed secret for `example` # with the SPIFFE ID # `\"spiffe://vsecm.com/workload/example\\ # /ns/{{ .PodMeta.Namespace }}\\ # /sa/{{ .PodSpec.ServiceAccountName }}\\ # /n/{{ .PodMeta.Name }}\"` name: vsecm-secret-example namespace: default type: Opaque Secondly this approach is less secure, and it is meant to be used for legacy systems where directly using the Safe Sidecar or Safe SDK are not feasible. For example, you might not have direct control over the source code to enable a tighter Safe integration. Or, you might temporarily want to establish behavior parity of your legacy system before starting a more canonical VMware Secrets Manager implementation. VSECM_SIDECAR_POLL_INTERVAL VSECM_SIDECAR_POLL_INTERVAL is the interval (in milliseconds) that the sidecar polls VSecM Safe for new secrets. Defaults to 20000 milliseconds, if not provided. VSECM_SIDECAR_MAX_POLL_INTERVAL VSecM Sidecar has an exponential backoff algorithm to execute fetch in longer intervals when an error occurs. VSECM_SIDECAR_MAX_POLL_INTERVAL is the maximum wait time (in milliseconds) before executing the next. Defaults to 300000 milliseconds, if not provided. VSECM_SIDECAR_EXPONENTIAL_BACKOFF_MULTIPLIER VSecM Sidecar uses this environment variable. VSECM_SIDECAR_EXPONENTIAL_BACKOFF_MULTIPLIER configures how fast the algorithm backs off when there is a failure. Defaults to 2, which means when there are enough failures to trigger a backoff, the next wait interval will be twice the current one. VSECM_SIDECAR_SUCCESS_THRESHOLD VSecM Sidecar uses this environment variable. VSECM_SIDECAR_SUCCESS_THRESHOLD configures the number of successful poll results before reducing the poll interval. Defaults to 3. The next interval is calculated by dividing the current interval with VSECM_SIDECAR_EXPONENTIAL_BACKOFF_MULTIPLIER. VSECM_SIDECAR_ERROR_THRESHOLD VSecM Sidecar uses this environment variable. VSECM_SIDECAR_ERROR_THRESHOLD configures the number of fetch failures before increasing the poll interval. Defaults to 2. The next interval is calculated by multiplying the current interval with VSECM_SIDECAR_EXPONENTIAL_BACKOFF_MULTIPLIER. VSECM_SYSTEM_NAMESPACE VSECM_SYSTEM_NAMESPACE environment variable specifies the namespace in which a VSecM instance is deployed. Ensure this is set as an environment variable for your containers; it’s a critical piece. VSecM Safe and Sentinel rely on it to precisely locate the deployment’s namespace. For instance, Safe leverages this information to securely store age keys within a designated secret, as specified by the VSECM_CRYPTO_KEY_NAME configuration."
  },"/docs/use-the-source/": {
    "title": "Building From Source",
    "keywords": "",
    "url": "/docs/use-the-source/",
    "body": "edit this page on GitHub ✏️ Introduction This section describes how to build VMware Secrets Manager from source. For a more detailed walkthrough about how to contribute to VMware Secrets Manager, see the Contributing section. Prerequisites Make you have the following installed on your system: make git docker Clone the Project cd $WORKSPACE git clone https://github.com/vmware-tanzu/secrets-manager.git cd secrets-manager Build the Project Make sure you have a running local Docker daemon and execute the following: make build-local That’s it 🎉. You now have images of VMware Secrets Manager and other related components built locally on your Docker registry. Next Up For a more detailed guide about how you can use these local container images in your custer check out the Contributing section."
  },"/docs/contributing/": {
    "title": "Contribute to VSecM",
    "keywords": "",
    "url": "/docs/contributing/",
    "body": "edit this page on GitHub ✏️ Introduction This section contains instructions to test and develop VMware Secrets Manager locally. 📚 Familiarize Yourself with the Contributing Guidelines Please make sure you read the Contributing Guidelines and the Code of Conduct on the VSecM GitHub repo first. Good First Issues for New Contributors If you are new to VMware Secrets Manager or looking for smaller tasks to start contributing, we have a set of issues labeled as “good first issue” on our GitHub repository. These issues are a great place to start if you are looking to make your first contribution. How to Find Good First Issues Navigate to the Issues tab in the GitHub repository. Use the label filter and select the “good first issue” label. Browse through the list and pick an issue that interests you. Claiming an Issue Before starting work on an issue, it’s a good practice to comment on it, stating that you intend to work on it. This prevents multiple contributors from working on the same issue simultaneously. Need Help? If you have questions or need further clarification on a “good first issue,” feel free to ask in the issue comments or reach out to the maintainers. Code Review Requirements While we value pragmatism over process, we do have some basic requirements for code reviews to ensure the quality and consistency of the codebase. Conducting Code Reviews Pull Requests: All code changes must be submitted through a pull request (PR) on GitHub. Minimum Reviews: Each PR must be reviewed by at least one other person before it can be merged. Open for Feedback: PRs are open for comments and suggestions from any team member, not just the designated reviewer. What Must Be Checked These are the minimum set of items that must be checked during a code review. More items may be checked depending on the nature of the change. Canonical Go: The code should adhere to canonical Go practices. Formatting: The code must pass gofmt without any issues. Consistency: The code should look like the rest of the codebase, as if it were written by a single individual. Acceptance Criteria Approval: At least one reviewer must approve the PR. Automated Checks: All automated tests and checks must pass. No Conflicts: Resolve any merge conflicts before merging. How to Conduct a Code Review Navigate to the Pull Requests tab in the GitHub repository. Choose a PR that is awaiting review. Review the code changes and provide your feedback, keeping the above criteria in mind. If the PR meets all the criteria, approve it; otherwise, request changes and provide constructive feedback. What Technologies Do I Need to Know? You don’t have to be an expert in all of these technologies to contribute to VMware Secrets Manager. However, being familiar with the following concepts and technologies will help you get started faster. Go VMware Secrets Manager is written in Go, so you should be familiar with the language and its idioms. If you are new to Go, we recommend going through the Go Tour and the Effective Go guide. Kubernetes VMware Secrets Manager is a Kubernetes-native application, so you should be familiar with the basics of Kubernetes. If you are new to Kubernetes, we recommend going through the Kubernetes Basics. Helm VMware Secrets Manager is packaged as a Helm chart, so you should be familiar with the basics of Helm. If you are new to Helm, we recommend going through the Helm Quickstart Guide. SPIFFE and SPIRE VMware Secrets Manager uses SPIFFE and SPIRE to establish an identity control plane, so you should be familiar with the basics of SPIFFE and SPIRE. If you are new to SPIFFE and SPIRE, we recommend going through the quickstart guides on the SPIFFE. go-spiffe VMware Secrets Manager uses the go-spiffe to interact with the SPIFFE and SPIRE APIs, so you should be familiar with the basics of go-spiffe. If you are new to go-spiffe, we recommend going through the documentation. ClusterSPIFFEID and ClusterFederatedTrustDomain VMware Secrets Manager uses the ClusterSPIFFEID and ClusterFederatedTrustDomain to dispatch identities to workloads, and federate cluster, respectively. If you are new to these concepts, we recommend you check out the SPIRE Controller Manager repository. VSecM Architecture VMware Secrets Manager has several components, each with its own responsibilities. Check out the architecture overview to get a high-level understanding of the components and their interactions. While you are at there, we strongly recommend going through the entire documentation to get a good understanding of the product. Docker We have several Dockerfiles in the repository, so you should be familiar with the basics of Docker. If you are new to Docker, we recommend going through the Docker Get Started guide."
  },"/docs/dev-env/": {
    "title": "Development Environment",
    "keywords": "",
    "url": "/docs/dev-env/",
    "body": "edit this page on GitHub ✏️ Introduction This section contains instructions to set up your development environment to actively contribute the VMware Secrets Manager project. Prerequisites Other than the source code, you need the following set up for your development environment to be able to locally develop VMware Secrets Manager: Docker installed and running for the local user. Minikube installed on your system. Make installed on your system. Git installed on your system. Can I Use Something Other than Minikube and Docker? Of course, you can use any Kubernetes cluster to develop, deploy, and test VMware Secrets Manager for Cloud-Native Apps. Similarly, you can use any OCI-compliant container runtime. It does not have to be Docker. We are giving Minikube and Docker as an example because they are easier to set up; and when you stumble upon, it is easy to find supporting documentation about these to help you out. For Mac OS, for example, you can check the alternate setup section below Alternate Non-Minikube Setup on Mac OS The rest of this document assumes that you are using Minikube and Docker; however, if you are on a Mac, want to use Docker for Mac’s Kubernetes Distribution, you can install them as described below and skip the sections that are specific to Minikube. Installing Docker for Mac Download and install Docker for Mac on your system. Enable Kubernetes on Docker for Mac Once you have Docker for Mac installed, you’ll need to enable Kubernetes on it. To do that, follow the steps below: Open Docker for Mac’s preferences. Go to the “Kubernetes” tab. Check the “Enable Kubernetes” checkbox. Disabling Airplay Receiver Airplay Receiver uses port 5000 by default, which is the same port that dokcer registry uses. To fix the first issue, on your Mac’s “System Settings” you’ll need to go to “Setting » Airdrop &amp; Handoff » Airplay Receiver” and on that screen… uncheck “allow handoff between this Mac and your iCloud devices”, make sure “Airdrop” is selected as “no one”. then you might need to kill the process that’s using port 5000 or restart your system. Alternatively, you can bind a different port to the docker registry. But when you do that, you’ll need to make sure that you update other files in the project too that reference localhost:5000. Install Docker Registry Use the following command to install the docker registry: docker run -d -p 5000:5000 --restart=always --name registry registry:2 You Are All Set And you should be all set. You can run make build-local to build local images, and make deploy-local to build and install VMware Secrets Manager locally. Cloning VMware Secrets Manager Create a workspace folder and clone VMware Secrets Manager into it. mkdir $HOME/Desktop/WORKSPACE cd $HOME/Desktop/WORKSPACE git clone \"https://github.com/vmware-tanzu/secrets-manager.git\" cd secrets-manager Want to Create a Pull Request? If you are contributing to the source code, make sure you read the contributing guidelines, and the code of conduct. Getting Help Running make help at the project root will provide you with a list of logically grouped commands. This page will not include the output because the content of the output can change depending on the version of VMware Secrets Manager; however, the output will give a nice overview of what you can do with the Makefile at the project root. make help Building, Deploying, and Testing Now let’s explain some of these steps (*and for the remainder, you can read the Makefile at the project root): make k8s-delete: Deletes your minikube cluster. make k8s-start: Starts an existing cluster, or creates a brand new one. make build-local: Builds all the projects locally and pushes them to the local container registry. make deploy-local: Deploys VMware Secrets Manager locally with the artifacts generated at the build-local step. make test-local: Runs integration tests to make sure that the changes that were made doesn’t break anything. If you run these commands in the above order, you’ll be able to build, deploy, and test your work locally. Minikube Quirks Docker for Mac Troubleshooting When you use Minikube with Docker for Mac, you’ll likely get a warning similar to the following: make k8s-start # below is the response to the command above ./hack/minikube-start.sh … … truncated … … Registry addon with docker driver uses port 50565 please use that instead of default port 5000 … The port 50656 is a randomly-selected port. Every time you run make k8s-start it will pick a different port. You can verify that the repository is there: curl localhost:50565/v2/_catalog # response: # {\"repositories\":[]} There are two issues here: First, all the local development scripts assume port 5000 as the repository port; however, port 5000 on your Mac will likely be controlled by the Airplay Receiver. And secondly, you’ll need to forward localhost:5000 to whatever port the error message shows you. To fix the first issue, on your Mac’s “System Settings” you’ll need to go to “Setting » Airdrop &amp; Handoff » Airplay Receiver” and on that screen… uncheck “allow handoff between this Mac and your iCloud devices”, make sure “Airdrop” is selected as “no one” and finally, after updating your settings, restart your Mac (this step is important; without restart, your macOS will still hold onto that port) Note that where these settings are can be slightly different from one version of macOS to another. As for the second issue, to redirect your local :5000 port to the docker engine’s designated port, you can use socat. # Install `socat` if you don’t have it on your system. brew install socat # Replace 49876 with whatever port the warning message # gave you during the initial cluster setup. socat TCP-LISTEN:5000,fork,reuseaddr TCP:localhost:49876 Then execute the following on a separate tab: curl localhost:5000/v2/_catalog # Should return something similar to this: # {\"repositories\":[]} If you get a successful response to the above curl, then congratulations, you have successfully set up your local docker registry for your VMware Secrets Manager development needs. Make a Big McTunnel If you have localhost:5000 unallocated, there is a make mac-tunnel target in the VMware Secrets Manager’s project Makefile that will automatically find the exposed docker registry port, and establish a tunnel for you. Execute this: make mac-tunnel And then on a separate terminal window, verify that you can access the registry from localhost:5000. curl localhost:5000/v2/_catalog # Should return something similar to this: # \"repositories\":[] Ubuntu Troubleshooting If you are using Ubuntu, it would be helpful to know that Minikube and snap version of Docker do not play well together. If you are having registry-related issues, or if you are not able to execute a docker images without being the root user, then one resolution can be to remove the snap version of docker and install it from the source: sudo apt update sudo apt-get install \\ apt-transport-https \\ ca-certificates \\ curl \\ gnupg-agent \\ software-properties-common curl -fsSL https://download.docker.com/linux/ubuntu/gpg | \\ sudo apt-key add - sudo apt-key fingerprint 0EBFCD88 sudo add-apt-repository \\ \"deb [arch=amd64] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) \\ stable\" sudo apt-get update sudo apt-get install docker-ce docker-ce-cli containerd.io Restart Your System After doing this, you might need to restart your system and execute minikube delete on your terminal too. Although you might feel that this step is optional, it is not; trust me 🙂. After installing a non-snap version of Docker and restarting your system, if you can use Minikube Docker registry, then, perfect. If not, there are a few things that you might want to try. So if you are still having issues keep on reading. Before trying anything else, it might be worth giving Docker Post Installation Steps from the official Docker website a shot. Following that guideline may solve Docker-related permission issues that you might still be having. Restart, Maybe? If you still have permission issues after following the official Docker post-installation steps outlined above, try restarting your computer once more. Especially when it comes to Docker permissions, restarting can help, and worst case it’s still worth giving a try. Still no luck? Keep on reading. Depending on your operating system, and the Minikube version that you use it might take a while to find a way to push images to Minikube’s internal Docker registry. The relevant section about the Minikube handbook covers a lot of details, and can be helpful; however, it is also really easy skip or overlook certain steps. If you have docker push issues, or you have problem your Kubernetes Pods acquiring images from the local registry, try these: Execute eval $(minikube docker-env) before pushing things to Docker. This is one of the first instructions on the “push” section of the Minikube handbook, yet it is still very easy to inadvertently skip it. Make sure you have the registry addon enabled (minikube addons list). You might have luck directly pushing the image: first: docker build --tag $(minikube ip):5000/test-img; followed by: docker push $(minikube ip):5000/test-img. There are also minikube image load and minikube image build commands that might be helpful. Enjoy 🎉 Just explore the Makefile and get a feeling of it. Feel free to touch base if you have any questions, comments, recommendations."
  },"/docs/production/": {
    "title": "Production Setup",
    "keywords": "",
    "url": "/docs/production/",
    "body": "edit this page on GitHub ✏️ Introduction You need to pay attention to certain aspects and parts of the system that you’d need to harden for a production VMware Secrets Manager setup. This article will overview them. Version Compatibility We test VMware Secrets Manager with the recent stable version of Kubernetes and Minikube. As long as there isn’t a change in the major version number your Kubernetes client and server you use, things will likely work just fine. Resource Requirements VMware Secrets Manager is designed from the ground up to work in environments with limited resources, such as edge computing and IoT. That being said, VMware Secrets Manager, by design, is a memory-intensive application. However, even when you throw all your secrets at it, VSecM Safe’s peak memory consumption will be in the order or 10-20 megabytes of RAM. The CPU consumption will be within reasonable limits too. However, it’s crucial to understand that every system and user profile is unique. Factors such as the number and size of secrets, concurrent processes, and system specifications can influence these averages. Therefore, it is always advisable to benchmark VMware Secrets Manager and SPIRE on your own system under your specific usage conditions to accurately gauge the resource requirements to ensure optimal performance. Benchmark your system usage and set CPU and Memory limits to the VSecM Safe pod. We recommend you to: Set a memory request and limit for VSecM Safe, Set a CPU request; but not set a CPU limit for VSecM Safe (i.e., the VSecM Safe pod will ask for a baseline CPU; yet burst for more upon need). As in any secrets management solution, your compute and memory requirements will depend on several factors, such as: The number of workloads in the cluster The number of secrets Safe (VMware Secrets Manager’ Secrets Store) has to manage (see architecture details for more context) The number of workloads interacting with Safe (see architecture details for more context) Sidecar poll frequency (see architecture details for more context) etc. We recommend you benchmark with a realistic production-like cluster and allocate your resources accordingly. That being said, here are the resource allocation reported by kubectl top for a demo setup on a single-node minikube cluster to give an idea: NAMESPACE WORKLOAD CPU(cores) MEMORY(bytes) vsecm-system vsecm-safe 1m 9Mi vsecm-system vsecm-sentinel 1m 3Mi default example 2m 7Mi spire-system spire-agent 4m 35Mi spire-system spire-server 6m 41Mi Note that 1000m is 1 full CPU core. Based on these findings, the following resource and limit allocations can be a starting point for VMware Secrets Manager-managed containers: # Resource allocation will highly depend on the system. # Benchmark your deployment, monitor your resource utilization, # and adjust these values accordingly. resources: requests: memory: \"128Mi\" cpu: \"250m\" limits: memory: \"128Mi\" # We recommend “NOT” setting a CPU limit. # As long as you have configured your CPU “requests” # correctly, everything would work fine. Back Up Your Cluster Regularly VMware Secrets Manager is designed to be resilient; however, losing access to your sensitive data is possible by inadvertently deleting a Kubernetes Secret that you are not supposed to delete. Or, your backing store that contains the secrets can get corrupted for any reason. Cloud Native or not, you rely on hardware which—intrinsically—is unreliable. Things happen. Make sure you back up your cluster using a tool like Velero, so that when things do happen, you can revert your cluster’s last known good state. Make Sure You Back Up vsecm-safe-age-key The Kubernetes Secret names vsecm-safe-age-key that resides in the vsecm-system namespace is especially important, and needs to be securely backed up. The reason is; if you lose this secret, you will lose access to all the encrypted secret backups, and you will not be able to restore your secrets. Set up your backups from day zero. Restrict Access To vsecm-safe-age-key The vsecm-safe-age-key secret that VSecM Safe stores in the vsecm-system namespace contains the keys to encrypt and decrypt secret data on the data volume of VSecM Safe. While reading the secret alone is not enough to plant an attack on the secrets (because the attacker also needs to access the VSecM Safe Pod or the /data volume in that Pod), it is still crucial to follow the principle of least privilege guideline and do not allow anyone on the cluster read or write to the vsecm-safe-age-key secret. The only entity allowed to have read/write (but not delete) access to vsecm-safe-age-key should be the VSecM Safe Pod inside the vsecm-system namespace with an vsecm-safe service account. With Great Power Comes Great Responsibility It is worth noting that a Cluster Administrator due to their elevated privileges can read/write to any Kubernetes Secret in the cluster. This includes access to the vsecm-safe-age-key secret. Therefore, it is highly recommended that you grant the cluster-admin role to a very small group of trusted individuals only. Although, access to vsecm-safe-age-key does not give the attacker direct access to the secrets, due to their sheer power, a determined Cluster Administrator can still read the secrets by accessing the /data volume. Their actions will be recorded in the audit logs, so they can, and will be held responsible; however, it is still a bad idea to have more than an absolute minimum number of Cluster Administrators in your cluster. Kubernetes Secrets are, by default, stored unencrypted in the API server’s underlying data store (etcd). Anyone with API access and sufficient RBAC credentials can retrieve or modify a Secret, as can anyone with access to etcd. Secretless VSecM For an additional layer of security, you can opt out of using Kubernetes Secrets altogether and use VMware Secrets Manager without any Kubernetes secrets to protect the master keys. In this mode, you’ll have to manually provide the master keys to VSecM Safe; and you’ll need to re-provide the master keys every time you restart the VSecM Safe Pod or the pod is evicted, crashed, or rescheduled. This added layer of security comes with a cost of added complexity and operational overhead. You will need to manually intervene when VSemM Safe crashes or restarts. That said, VSecM Safe is designed to be resilient, and it rarely crashes. If you let VMware Secrets Manager generate the root token for you, you will not have to worry about this, and when the system crashes, it will automatically unlock itself, so you can #sleepmore. Our honest recommendation is to let VMware Secrets Manager manage your keys unless you have special conformance or compliance requirements that necessitate you to do otherwise. Check ou the Configuration Reference for more information. If you are only using VMware Secrets Manager for your configuration and secret storage needs, and your workloads do not bind any Kubernetes Secret (i.e., instead of using Kubernetes Secret objects, you use tools like VSecM SDK or VSecM Sidecar to securely dispatch secrets to your workloads) then as long as you secure access to the secret vsecm-safe-age-key inside the vsecm-system namespace, you should be good to go. With the help of VSecM SDK, VSecM Sidecar, and VSecM Init Container, and with some custom coding/shaping of your data, you should be able to use it. However, VMware Secrets Manager also has the option topersist the secrets stored in VSecM Safe as Kubernetes Secret objects. This approach can help support legacy systems where you want to start using VMware Secrets Manager without introducing much code and infrastructure change to the existing cluster—at least initially. If you are using VMware Secrets Manager to generate Kubernetes Secrets for the workloads to consume, then take regular precautions around those secrets, such as implementing restrictive RBACs, and even considering using a KMS to encrypt etcd at rest if your security posture requires it. Do I Really Need to Encrypt etcd? tl;dr: Using plain Kubernetes Secrets is good enough, and it is not the end of the world if you keep your etcd unencrypted. VMware Secrets Manager Keeps Your Secrets Safe If you use VMware Secrets Manager to store your sensitive data, your secrets will be securely stored in VSecM Safe (instead of etcd), so you will have even fewer reasons to encrypt etcd 😉. Details This is an excellent question. And as in any profound answer to good questions, the answer is: “it depends” 🙂. Secrets are, by default, stored unencrypted in etcd. So if an adversary can read etcd in any way, it’s game over. Threat Analysis Here are some ways this could happen: Root access to a control plane node. Root access to a worker node. Access to the actual physical server (i.e., physically removing the disk). Possible zero day attacks. For 1, and 2, server hardening, running secure Linux instances, patching, and preventing privileged pods from running in the cluster are the usual ways to mitigate the threat. Unfortunately, it is a relatively complex attack vector to guard against. Yet, once your node is compromised, you have a lot of things to worry about. In that case, etcd exposure will be just one of many, many, many concerns that you’ll have to worry about. For 3, assuming your servers are in a data center, there should already be physical security to secure your servers. So the attack is unlikely to happen. In addition, your disks are likely encrypted, so unless the attacker can shell into the operating system, your data is already safe: Encrypting etcd once more will not provide any additional advantage in this particular case, given the disk is encrypted, and root login is improbable. For 4., the simpler your setup is, the lesser moving parts you have, and the lesser the likelihood of bumping into a zero-day. And Kubernetes Secrets are as simple as it gets. Even when you encrypt etcd at rest using a KMS (which is the most robust method proposed in the Kubernetes guides), an attacker can still impersonate etcd and decrypt the secrets: As long as you provide the correct encrypted DEK to KMS, the KMS will be more than happy to decrypt that DEK with its KEK and provide a plain text secret to the attacker. Secure Your House Before Securing Your Jewelry So, yes, securing etcd will marginally increase your security posture. Yet, it does not make too much of a difference unless you have already secured your virtual infrastructure and physical data center. And if you haven’t secured your virtual and physical assets, then you are in big trouble at day zero, even before you set up your cluster, so encrypting etcd will not save you the slightest from losing other valuable data elsewhere anyway. Security Is a Layered Cake That being said, we are humans, and $#!% does happen: If a node is compromised due to a misconfiguration, it would be nice to make the job harder for the attacker. Restrict Access to VSecM Sentinel All VMware Secrets Manager images are based on distroless containers for an additional layer of security. Thus, an operator cannot execute a shell on the Pod to try a privilege escalation or container escape attack. However, this does not mean you can leave the vsecm-system namespace like an open buffet. Always take a principle of least privilege stance. For example, do not let anyone who does not need to fiddle with the vsecm-system namespace see and use the resources there. This stance is especially important for the VSecM Sentinel Pod since an attacker with access to that pod can override (but not read) secrets on workloads. VMware Secrets Manager leverages Kubernetes security primitives and modern cryptography to secure access to secrets. And VSecM Sentinel is the only system part that has direct write access to the VSecM Safe secrets store. Therefore, once you secure access to VSecM Sentinel with proper RBAC and policies, you secure access to your secrets. Volume Selection for VSecM Safe Backing Store VSecM Safe default deployment descriptor uses HostPath to store encrypted backups for secrets. It is highly recommended to ensure that the backing store VSecM Safe uses is durable, performant, and reliable. It is a best practice to avoid HostPath volumes for production deployments. You are strongly encouraged to choose a PersistentVolume that suits your needs for production setups. High Availability of VSecM Safe tl;dr: VSecM Safe may not emphasize high-availability, but its robustness is so outstanding that the need for high-availability becomes almost negligible. Since VSecM Safe keeps all of it state in memory, using a pod with enough memory and compute resources is the most effective way to leverage it. Although, with some effort, it might be possible to make it highly available, the effort will likely bring unnecessary complexity without much added benefit. VSecM Safe is, by design, a single pod; so technically-speaking, it is not highly-available. So in the rare case when VSecM Safe crashes, or gets evicted due to a resource contention, there will be minimal disruption until it restarts. However, VSecM Safe restarts fairly quickly, so the time window where it is unreachable will hardly be an issue. Moreover VSecM Safe employs “lazy learning” and does not load everything into memory all at once, allowing very fast restarts. In addition, its lightweight and focused code ensures that crashes are infrequent, making VSecM Safe practically highly available. While it is possible to modify the current architecture to include more than one VSecM Safe pod and place it behind a service to ensure high-availability, this would be a significant undertaking, with not much benefit to merit it: First of all, for that case to happen, the state would need to be moved away from the memory, and centralized into a common in-memory store (such as Redis, or etcd). This will introduce another moving part to manage. Or alternatively all VSecM Safe pods could be set up to broadcast their operations and reach a quorum. A quorum-based solution would be more complex than using a share store, besides reaching a quorum means a performance it (both in terms of decision time and also compute required). On top of all these bootstrapping coordination would be necessary to prevent two pods from creating different bootstrap secrets simultaneously. Also, for a backing store like Redis, the data would need to be encrypted (and Redis, for example, does not support encryption at rest by default). When considering all these, VSecM Safe has not been created highly-available by design; however, it is so robust, and it restarts from crashes so fast that it’s “as good as” highly-available. Update VMware Secrets Manager’s Log Levels VSecM Safe and VSecM Sidecar are configured to log at TRACE level by default. This is to help you debug issues with VMware Secrets Manager. However, this can cause a lot of noise in your logs. Once you are confident that VMware Secrets Manager works as expected, you can reduce the log level to INFO or WARN. For this, you will need to modify the VSECM_LOG_LEVEL environment variable in the VSecM Safe and VSecM Sidecar Deployment manifests. See Configuring VMware Secrets Manager for details. Conclusion Since VMware Secrets Manager is a Kubernetes-native framework, its security is strongly related to how you secure your cluster. You should be safe if you keep your cluster and the vsecm-system namespace secure and follow “the principle of least privilege” as a guideline. VMware Secrets Manager is a lightweight secrets manager; however, that does not mean it runs on water: It needs CPU and Memory resources. The amount of resources you need will depend on the criteria outlined in the former sections. You can either benchmark your system and set your resources accordingly. Or set generous-enough limits and adjust your settings as time goes by. Also, you are strongly encouraged not to set a limit on VMware Secrets Manager Pods’ CPU usage. Instead, it is recommended to let VSecM Safe burst the CPU when it needs. On the same topic, you are encouraged to set a request for VSecM Safe to guarantee a baseline compute allocation."
  },"/docs/use-cases/": {
    "title": "Use Cases",
    "keywords": "",
    "url": "/docs/use-cases/",
    "body": "edit this page on GitHub ✏️ Introduction 🐢 This document lists various use cases to register secrets to Kubernetes workloads using VMware Secrets Manager, in tutorial form. Each tutorial isolated in itself, explaining a specific feature of VMware Secrets Manager. Following these tutorials, you will have a better understanding of what VMware Secrets Manager is capable of, you will learn core VMware Secrets Manager concepts by doing. When you complete the tutorials listed here, you will have a fair understanding of how to use VMware Secrets Manager to manage your secrets. Follow the White Rabbit 🐇 We advise you to follow these tutorials in the sequence they are presented here. We’ve structured them this way to start with simpler use cases and progressively introduce more advanced techniques as we build upon our knowledge. Overview and Prerequisites Using VSecM Sidecar Using VSecM SDK Using VSecM Init Container Encrypting Secrets Transforming Secrets VMware Secrets Manager Showcase Further Reading The use cases above leverage VSecM Sentinel and VSecM SDK. For the interested, the following sections cover these tools in greater detail: VSecM SDK Documentation VSecM Sentinel Command Line Reference"
  },"/docs/use-cases-overview/": {
    "title": "Overview",
    "keywords": "",
    "url": "/docs/use-cases-overview/",
    "body": "edit this page on GitHub ✏️ Introduction This section provides the necessary prerequisites for the upcoming tutorials, as well as a high-level overview of the architecture. This information should suffice to get you started and familiar with the basics. Prerequisites To complete the tutorials listed here, you will need the following: A Kubernetes cluster that you have sufficient admin rights. VMware Secrets Manager up and running on that cluster. The vmware-tanzu/secrets-manager repository cloned inside a workspace folder (such as /home/WORKSPACE/secrets-manager) How Do I Set Up VMware Secrets Manager? To set up VMware Secrets Manager, follow the instructions in this quickstart guide. Minikube Instructions For your Kubernetes cluster, you can use minikube for development purposes. To use minikube, as your cluster, make sure you have Docker up and running first—while there are other ways, using Minikube’s Docker driver is the fastest and painless way to get started. Once you have Docker up and running, execute the following script to install minikube. Note that you will also need git and make installed on your system. # Switch to your workspace folder (e.g., `~/Desktop/WORKSPACE`). cd $WORKSPACE # Clone VMware Secrets Manager repository if you haven’t already done so: git clone https://github.com/vmware-tanzu/secrets-manager.git # cd into the cloned project folder cd secrets-manager # Test if `make` is working, if it fails, install `make` first make help # Install minikube make k8s-start Can I Use This Other Thing Instead? You can of course use other tools such as microk8s, or kind, k38 or even a full-blown managed Kubernetes cluster; however it will be virtually impossible to cover all possible tooling and OS combinations. Therefore, we’ll only provide instructions for minikube in this document. High-Level Overview Here is a high-level overview of various components that will interact with each other in the upcoming tutorials: On the above diagram: SPIRE is the identity provider for all intents and purposes. VSecM Safe is where secrets are stored. VSecM Sentinel can be considered a bastion host. Demo Workload is a typical Kubernetes Pod that needs secrets. Want a Deeper Dive? In this tutorial, we cover only the amount of information necessary to follow through the steps and make sense of how things tie together from a platform operator’s perspective. You can check out this “VMware Secrets Manager Deep Dive” article to learn more about these components. The Demo Workload fetches secrets from VSecM Safe. This is either indirectly done through a sidecar or directly by using VMware Secrets Manager Go SDK. Using VSecM Sentinel, an admin operator or ar CI/CD pipeline can register secrets to VSecM Safe for the Demo Workload to consume. All the above workload-to-safe and sentinel-to-safe communication are encrypted through mTLS using the X.509 SVIDs that SPIRE dispatches to all the actors. After this high-level overview of your system, let’s create a workload next."
  },"/docs/use-case-sidecar/": {
    "title": "Using VSecM Sidecar",
    "keywords": "",
    "url": "/docs/use-case-sidecar/",
    "body": "edit this page on GitHub ✏️ Using With VSecM Sidecar Let’s deploy our demo workload that will use VSecM Sidecar. You can find the deployment manifests inside the ./examples/workload-using-sidecar/k8s folder of your cloned VMware Secrets Manager folder. To deploy our workload using that manifest, execute the following: # Switch to the VSecM repo: cd $WORKSPACE/secrets-manager # Install the workload: make example-sidecar-deploy # If you are building from the source, # use `make example-sidecar-deploy-local` instead. And that’s it. You have your demo workload up and running. Read the Source Make sure you examine the manifests to gain an understanding of what kinds of entities you’ve deployed to your cluster. You’ll see that there are two images in the Deployment object declared inside that folder: vsecm/example: This is the container that has the business logic. vsecm/vsecm-ist-sidecar: This VMware Secrets Manager-managed container injects secrets to a place that our demo container can consume. The Demo App Here is the source code of the demo container’s app for the sake of completeness. When you check the source code, you’ll see that our demo app tries to read a secret file every 5 seconds forever: for { dat, err := os.ReadFile(sidecarSecretsPath()) if err != nil { fmt.Println(\"Failed to read. Will retry in 5 seconds…\") fmt.Println(err.Error()) } else { fmt.Println(\"secret: '\", string(dat), \"'\") } time.Sleep(5 * time.Second) } ClusterSPIFFEID Yet, how do we tell VMware Secrets Manager about our app so that it can identify it to deliver secrets? For this, there is an identity file that defines a ClusterSPIFFEID for the workload: # ./examples/workload-using-sidecar/k8s/Identity.yaml kind: ClusterSPIFFEID metadata: name: example spec: # SPIFFE ID `MUST` start with # \"spiffe://vsecm.com/workload/$workloadName/ns/\" # for `safe` to recognize the workload and # dispatch secrets to it. spiffeIDTemplate: \"spiffe://vsecm.com\\ /workload/example\\ /ns/{{ .PodMeta.Namespace }}\\ /sa/{{ .PodSpec.ServiceAccountName }}\\ /n/{{ .PodMeta.Name }}\" podSelector: matchLabels: app.kubernetes.io/name: example workloadSelectorTemplates: - \"k8s:ns:default\" - \"k8s:sa:example\" This identity descriptor, tells VMware Secrets Manager that the workload: Lives under a certain namespace, Is bound to a certain service account, And as a certain name. When the time comes, VMware Secrets Manager will read this identity and learn about which workload is requesting secrets. Then it can decide to deliver the secrets (because the workload is registered) or deny dispatching them (because the workload is unknown/unregistered). ClusterSPIFFEID is an Abstraction Please note that Identity.yaml is not a random YAML file: It is a binding contract and abstracts a host of operations behind the scenes. For every ClusterSPIFFEID created this way, SPIRE (VSecM’ identity control plane) will deliver an X.509 SVID bundle to the workload. Therefore, creating a ClusterSPIFFEID is a way to irrefutably, securely, and cryptographically identify a workload. Verifying the Deployment If you have been following along so far, when you execute kubectl get po will give you something like this: kubectl get po NAME STATUS AGE example-5d564458b6-vsmtm 2/2 Running 9s Let’s check the logs of our pod: kubectl logs example-5d564458b6-vsmtm -f The output will be something like this: Failed to read the secrets file. Will retry in 5 seconds… open /opt/vsecm/secrets.json: no such file or directory Failed to read the secrets file. Will retry in 5 seconds… open /opt/vsecm/secrets.json: no such file or directory Failed to read the secrets file. Will retry in 5 seconds… … What we see here that our workload checks for the secrets file and cannot find it for a while, and displays a failure message. Registering a Secret Let’s register a secret and see how the logs change: # Find the name of the VSecM Sentinel pod. kubectl get po -n vsecm-system # register a secret to our workload using VSecM Sentinel kubectl exec vsecm-sentinel-778b7fdc78-86v6d -n vsecm-system \\ -- safe \\ -w \"example\" \\ -s \"VSecMRocks!\" # Response: # OK Now let’s check the logs again: kubectl logs example-5d564458b6-vsmtm -f secret: ' VSecMRocks! ' secret: ' VSecMRocks! ' secret: ' VSecMRocks! ' secret: ' VSecMRocks! ' … So we registered our first secret to a workload using VSecM Sentinel. The secret is stored in VSecM Safe and dispatched to the workload through VSecM Sidecar behind the scenes. What Is VSecM Sentinel? For all practical purposes, you can think of VSecM Sentinel as the “bastion host” you log in and execute sensitive operations. In our case, we will register secrets to workloads using it. Registering Multiple Secrets If needed, you can associate more than one secret to a worklad, for this, you’ll need to use the -a (for “append”) flag. kubectl exec vsecm-sentinel-778b7fdc78-86v6d -n vsecm-system \\ -- safe \\ -w \"example\" \\ -s \"VSecMRocks!\" \\ -a # Response: # OK kubectl exec vsecm-sentinel-778b7fdc78-86v6d -n vsecm-system \\ -- safe \\ -w \"example\" \\ -s \"YouRockToo!\" \\ -a # Response: # OK Now, let’s check our logs: k logs example-5d564458b6-sx9sj -f secret: ' [\"YouRockToo!\",\"VSecMRocks!\"] ' secret: ' [\"YouRockToo!\",\"VSecMRocks!\"] ' secret: ' [\"YouRockToo!\",\"VSecMRocks!\"] ' secret: ' [\"YouRockToo!\",\"VSecMRocks!\"] ' Yes, we have two secrets in an array. VSecM Safe returns a single string if there is a single secret associated with the workload, and a JSON Array of strings if the workload has more than one secret registered. More About ClusterSPIFFEID Let’s dig a bit deeper. ClusterSPIFFEID is a Kubernetes Custom Resource that enables distributing SPIRE identities to workloads in a cloud-native and declarative way. Assuming you’ve had a chance to review the deployment manifests as recommended at the start of this tutorial, you might have noticed something similar to what’s presented below in the Identity.yaml.” spiffeIDTemplate: \"spiffe://vsecm.com\\ /workload/example\\ /ns/{{ .PodMeta.Namespace }}\\ /sa/{{ .PodSpec.ServiceAccountName }}\\ /n/{{ .PodMeta.Name }}\" The example part from that template is the name that VMware Secrets Manager will identify this workload as. That is the name we used when we registered the secret to our workload. VSecM Sentinel Commands You can execute kubectl exec -it $sentinelPod -n vsecm-system -- safe --help for a list of all available commands and command-line flags that VSecM Sentinel has. Also, Check out VSecM Sentinel CLI Reference for more information and usage examples on VSecM Sentinel. Conclusion Yay 🎉. That was our first secret. In the next tutorial, we will do something similar; however, this time we will leverage VSecM SDK instead of VSecM Sidecar."
  },"/docs/use-case-sdk/": {
    "title": "VSecM Developer SDK",
    "keywords": "",
    "url": "/docs/use-case-sdk/",
    "body": "edit this page on GitHub ✏️ Using VSecM SDK Now, let’s programmatically consume the VSecM Safe API from our workload. That way, you will have more control over how you consume and cache your secrets, and you will not need to add a sidecar to your pod. Cleanup Let’s remove the workload first: kubectl delete deployment example That will get rid of the workload; but, assuming you have completed the tutorial before this one, you’ll still have a secret registered. Let’s see it: # Find the sentinel pod’s name: kubectl get po -n vsecm-system # List secrets: kubectl exec vsecm-sentinel-778b7fdc78-86v6d -n \\ vsecm-system -- safe -l {\"secrets\":[{\"name\":\"example\", \"created\":\"Sat May 13 20:42:20 +0000 2023\", \"updated\":\"Sat May 13 20:42:20 +0000 2023\"}]} Let’s delete it first: kubectl exec vsecm-sentinel-778b7fdc78-86v6d -n \\ vsecm-system -- safe -w example -d OK And make sure that it is gone: kubectl exec vsecm-sentinel-778b7fdc78-86v6d -n \\ vsecm-system -- safe -l # Output: # {\"secrets\":[]} All right, our cluster is as clean as a baby’s butt; let’s move on. Read the Source Make sure you examine the manifests to gain an understanding of what kinds of entities you’ve deployed to your cluster. The Benefit of Using VSecM SDK VSecM SDK gives direct control of VSecM Safe to your workload. The advantage of this approach is: you are in charge. The downside of it is: Well, you are in charge 🙂. But, jokes aside, your application will have to be more tightly bound to VMware Secrets Manager without a sidecar. However, when you use a sidecar, your application does not have any idea of VMware Secrets Manager’s existence. From its perspective, it is merely reading from a file that something magically updates every once in a while. This “separation of concerns” can make your application architecture more adaptable to changes. As in anything, there is no one true way to do it. Your approach will depend on your project’s requirements. Deploying the Demo Workload That part taken care of; let’s deploy a workload that uses VSecM SDK instead of VSecM Sidecar. # Switch to the VMware Secrets Manager repo: cd $WORKSPACE/secrets-manager # Install the workload: make example-sdk-deploy # If you are building from the source, # use `make example-sdk-deploy-local` instead. And that’s it. You have your demo workload up and running. The Demo App Here is the source code of the demo container’s app for the sake of completeness. When you check the source code, you’ll see that our demo app tries to get the secret by querying the SDK via sentry.Fetch(), displays the secret if it finds and repeats this every 5 seconds in an infinite loop. for { log.Println(\"fetch\") d, err := sentry.Fetch() // … (error handling) truncated … fmt.Printf( \"secret: updated: %s, created: %s, value: %s\\n\", d.Updated, d.Created, d.Data, ) time.Sleep(5 * time.Second) } Verifying the Deployment If you have been following along so far, when you execute kubectl get po will give you something like this: kubectl get po NAME READY AGE example-85bdbc4cf4-6n2ng 1/1 Running 9s Let’s check the logs of our pod: kubectl logs example-85bdbc4cf4-6n2ng -f The output should be something like this: 2023/07/31 20:17:13 fetch Failed to read the secrets file. Will retry in 5 seconds… Secret does not exist 2023/07/31 20:17:19 fetch Failed to read the secrets file. Will retry in 5 seconds… Secret does not exist … We don’t have any secrets registered to our workload as expected. So, let’s add some. Registering a Secret Let’s register a secret and see how the logs change: # Find the name of the VSecM Sentinel pod: kubectl get po -n vsecm-system # register a secret to our workload using VSecM Sentinel: kubectl exec vsecm-sentinel-778b7fdc78-86v6d -n vsecm-system \\ -- safe \\ -w \"example\" \\ -s \"VSecMRocks!\" # Response: # OK Now let’s check the logs again: kubectl logs example-85bdbc4cf4-6n2ng -f 2023/07/31 20:21:06 fetch secret: updated: \"2023-07-31T20:21:03Z\", created: \"2023-07-31T20:21:03Z\", value: VSecMRocks! … Conclusion So, yay 🎉. We got our secret; and since we use VSecM SDK, we also were able to get additional metadata such as the creation and modification timestamps of our secret, which was not possible to retrieve if we used the VSecM Sidecar approach that we have seen in the former tutorial. Next up, you’ll learn about VSecM Init Container."
  },"/docs/use-case-init-container/": {
    "title": "VSecM Init Container",
    "keywords": "",
    "url": "/docs/use-case-init-container/",
    "body": "edit this page on GitHub ✏️ Using VSecM Init Container In certain situations you might not have full control over the source code of your workloads. For example, your workload can be a containerized third party binary executable that you don’t have the source code of. It might be consuming Kubernetes Secrets through injected environment variables, and the like. Luckily, with VSecM Init Container you can interpolate secrets stored in VSecM Safe to the Data section of Kubernetes Secrets at runtime to be consumed by the workloads. ☝️ This sounds a bit mouthful. Fear not: Everything will be crystal clear after you go through this tutorial. Cleanup Let’s remove our workload and its associated secret to start with a clean slate: # Remove the workload deployment: kubectl delete deployment example # Find the sentinel pod’s name: kubectl get po -n vsecm-system # Delete the secret: kubectl exec vsecm-sentinel-778b7fdc78-86v6d -n \\ vsecm-system -- safe -w example -d # Make sure that the secret is gone: kubectl exec vsecm-sentinel-778b7fdc78-86v6d -n \\ vsecm-system -- safe -l # Output: # {\"secrets\":[]} Read the Source Make sure you examine the manifests to gain an understanding of what kinds of entities you’ve deployed to your cluster. Demo Workload Here are certain important code pieces from the demo workload that we are going to deploy soon. The following is the main application that the workload runs: // ./examples/workload-using-init-container/main.go func main() { // … Truncated … for { fmt.Printf(\"My secret: '%s'.\\n\", os.Getenv(\"SECRET\")) fmt.Printf(\"My creds: username:'%s' password:'%s'.\\n\", os.Getenv(\"USERNAME\"), os.Getenv(\"PASSWORD\"), ) fmt.Println(\"\") time.Sleep(5 * time.Second) } } As you see, the code tries to parse several environment variables. But, where does it get them? For that let’s look into the Deployment.yaml manifest: apiVersion: apps/v1 kind: Deployment metadata: name: example namespace: default labels: app.kubernetes.io/name: example spec: replicas: 1 selector: matchLabels: app.kubernetes.io/name: example template: metadata: labels: app.kubernetes.io/name: example spec: serviceAccountName: example containers: - name: main image: vsecm/example-using-init-container:latest env: - name: SECRET valueFrom: secretKeyRef: name: vsecm-secret-example key: VALUE - name: USERNAME valueFrom: secretKeyRef: name: vsecm-secret-example key: USERNAME - name: PASSWORD valueFrom: secretKeyRef: name: vsecm-secret-example key: PASSWORD # … Truncated … In the deployment manifest, there are environment variable bindings from a secret named vsecm-secret-example. Let’s look into that secret too: apiVersion: v1 kind: Secret metadata: name: vsecm-secret-example namespace: default type: Opaque As you see, the secret doesn’t have any data associated with it. We will dynamically populate it using VSecM Sentinel soon. Deploy the Demo Workload To begin, let’s deploy our demo workload: # Switch to the project folder: cd $WORKSPACE/secrets-manager # Deploy the demo workload: # Install the workload: make example-init-container-deploy # If you are building from the source, # use `make example-init-container-deploy-local` instead. When we list the pods, you’ll see that it’s not ready yet because VSecM Init Container is waiting for a secret to be registered to this pod. kubectl get po NAME READY STATUS example-5d8c6c4865-dlt8r 0/1 Init:0/1 0 Here are the containers in that Deployment.yaml containers: - name: main image: vsecm/example-using-init-container:latest # … Truncated … initContainers: - name: init-container image: vsecm/vsecm-ist-init-container:latest It’s the init-container that waits until the workload acquires a secret. Registering Secrets to the Workload To make the init container exit successfully and initialize the main container of the Pod, execute the following script: # ./examples/workload-using-init-container/register.sh # Find a Sentinel node. SENTINEL=$(kubectl get po -n vsecm-system \\ | grep \"vsecm-sentinel-\" | awk '{print $1}') # Execute the command needed to interpolate the secret. kubectl exec \"$SENTINEL\" -n vsecm-system -- safe \\ -w \"example\" \\ -n \"default\" \\ -s '{\"username\": \"root\", \\ \"password\": \"SuperSecret\", \"value\": \"VSecMRocks\"}' \\ -t '{\"USERNAME\":\"{{.username}}\", \\ \"PASSWORD\":\"{{.password}}\", \"VALUE\": \"{{.value}}\"}' \\ -k # Sit back and relax. Here are the meanings of the parameters in the above command: -w is the name of the workload. -n identifies the namespace of the Kubernetes Secret. -k means VMware Secrets Manager will update an associated Kubernetes Secret. -t is the template to be used to transform the fields of the payload. -s is the actual value of the secret. Now let’s check if our pod has initialized: kubectl get po NAME READY STATUS example-5d8c6c4865-dlt8r 1/1 Running 0 It looks like it did. So, let’s check its logs: kubectl logs example-5d8c6c4865-dlt8r My secret: 'VSecMRocks'. My creds: username:'root' password:'SuperSecret'. My secret: 'VSecMRocks'. My creds: username:'root' password:'SuperSecret'. My secret: 'VSecMRocks'. My creds: username:'root' password:'SuperSecret'. Which means, our secret should also have been populated; let’s check tha too: kubectl get secret NAME TYPE DATA AGE vsecm-secret-example Opaque 3 7h9m kubectl describe secret vsecm-secret-example Name: vsecm-secret-example Namespace: default Labels: &lt;none&gt; Annotations: &lt;none&gt; Type: Opaque Data ==== PASSWORD: 11 bytes USERNAME: 4 bytes VALUE: 10 bytes And yes, the values have been dynamically bound to the secret. What Happened? In summary, the Pod that your Deployment manages will not initialize until you register secrets to your workload. Once you register secrets using the above command, VSecM Init Container will exit with a success status code and let the main container initialize with the updated Kubernetes Secret. Here is a sequence diagram of how the secret is transformed (open the image in a new tab for a larger version): Conclusion That’s how you can register secrets as environment variables to workloads and halt bootstrapping of the main container until the secrets are registered to the workload. This approach is marginally less secure, because it creates interim secrets which are not strictly necessary if we were to use VSecM Sidecar or VSecM Safe. It is meant to be used for legacy systems where directly using the Safe Sidecar or Safe SDK are not feasible. For example, you might not have direct control over the source code to enable a tighter Safe integration. Or, you might temporarily want to establish behavior parity of your legacy system before starting a more canonical VMware Secrets Manager implementation. For modern workloads that you have more control, we highly encourage you to use VSecM SDK or VSecM Sidecar instead. That being said, it’s good to have this option, and we are sure you can find other creative ways to leverage it too. Next, we’ll learn how to encrypt secrets for safe storage."
  },"/docs/use-case-encryption/": {
    "title": "VSecM Encryption",
    "keywords": "",
    "url": "/docs/use-case-encryption/",
    "body": "edit this page on GitHub ✏️ Introduction This tutorial will introduce how you can use VSecM Sentinel encrypt secrets for safe keeping outside your cluster. What Is the Benefit? Since the secret will be encrypted, you can freely share it, and store in source control systems. When you’re ready to submit a secret to the workload, rather than providing the secret in plain text, you can deliver its encrypted version to VSecM Safe. This method offers a couple of distinct benefits: Firstly, it increases your overall security. Secondly, it allows for role differentiation: The individual (or process) who submits the secret doesn’t have to know its actual content; instead, they work with the encrypted version. Consequently, even if an impostor tries to mimic this individual, they wouldn’t be able to decipher the secret’s true value, drastically reducing potential avenues for attack. About the Encryption Process Please note that the encryption process and its inner workings remain mostly hidden to the end-user, ensuring a user-friendly experience. The process employs asymmetric encryption, where the secret is encrypted with a public key and decrypted using a private key by VSecM Safe. However, this is an implementation detail which can be subject to change. Cleanup Let’s remove the workload as usual: kubectl delete deployment example Next, delete the secret associated with this workload: # Find the sentinel pod’s name: kubectl get po -n vsecm-system # Delete secrets: kubectl exec vsecm-sentinel-778b7fdc78-86v6d -n \\ vsecm-system -- safe -w example -d OK That should be enough cleanup for the next steps. Introducing VSecM Inspector We will use VSecM Inspector like a debugger, to diagnose the state of our system. By the time of this writing VSecM Inspector is not an official VMware Secrets Manager component, so we’ll piggyback on a Deployment manifest that was used in a former workshop. When we have an vsecm-inspector pod that we can officially use for diagnostic purposes, this paragraph will be edited to reflect that too. Yet, for now, let’s deploy the workshop version of it. # Switch to the VMware Secrets Manager repo: cd $WORKSPACE/secrets-manager # Install VSecM Inspector: cd examples/pre-vsecm-workshop/inspector kubectl apply -f ServiceAccount.yaml kubectl apply -k . # Register VSecM Inspector’s ClusterSPIFFEID cd ../ids kubectl apply -f Inspector.yaml Now let’s test it: INSPECTOR=$(kubectl get po -n default \\ | grep \"vsecm-inspector-\" | awk '{print $1}') kubectl exec $INSPECTOR -- ./env # Output: # Failed to fetch the secrets. Try again later. # Secret does not exist Encrypting a Secret Now, let’s encrypt a secret using VSecM Sentinel: export SENTINEL=$(kubectl get po -n vsecm-system \\ | grep \"vsecm-sentinel-\" | awk '{print $1}') kubectl exec $SENTINEL -n vsecm-system -- safe \\ -s \"VSecMRocks\" \\ -e # The output of the above command will be # similar to something like this: # # YWdlLWVuY … Truncated … VZ2SDFiMjEY+V7JMg # # ☝️ This is a long random encrypted string. # We will use the variable $ENCRYPTED_SECRET in lieu of # this value in the sections below for simplicity. Here -s is for the secret we would like to encrypt, and -e indicates that we are not going to store the secret (yet), instead we want VSecM Sentinel to output the encrypted value of the secret to us. Registering the Encrypted Secret To register an encrypted secret, we use the -e flag to indicate that the secret is not plain text, and it is encrypted. kubect exec $SENTINEL -n vsecm-system -- safe \\ -w example \\ -s \"$ENCRYPTED_SECRET\" \\ -e And finally let’s inspect and see if the secret is registered properly: kubectl exec $INSPECTOR -- ./env # Will return \"VSecMRocks\" And yes, it did. Be Aware of the vsecm-safe-age-key Kubernetes Secret One thing to note is, if you lose access to the Kubernetes Secret named vsecm-safe-age-key in vsecm-system namespace, then you will lose the ability to register your encrypted secrets (since, during bootstrapping when VSecM Safe cannot find the secret, it will create a brand new one, invalidating all encrypted values). As a rule of thumb, always backup your cluster regularly, so that if such an incident occurs, you can recover the vsecm-safe-age-key secret from the backups. Conclusion This tutorial demonstrated how you can encrypt a secret value and register the encrypted value to VSecM Safe instead of the plain text secret. This technique provides and added layer of protection, and also allows you to safe the secret anywhere you like including source control systems. Next up, you’ll learn about secret transformations."
  },"/docs/use-case-transformation/": {
    "title": "Secret Transformation",
    "keywords": "",
    "url": "/docs/use-case-transformation/",
    "body": "edit this page on GitHub ✏️ Introduction This tutorial will show various way you can interpolate and transform secrets. Transforming secrets may come in handy when your workload expects the secret in a different format than it has been initially provided, and you don’t want to write custom code to do the transformation. To help us explore these transformations, we will use VSecM Inspector from the previous tutorial. If you haven’t installed it, before you proceed, please navigate to that lecture and install VSecM Inspector Preparation Let us define a few aliases first, they will speed things up: SENTINEL=$(kubectl get po -n vsecm-system \\ | grep \"vsecm-sentinel-\" | awk '{print $1}') SAFE=$(kubectl get po -n vsecm-system \\ | grep \"vsecm-safe-\" | awk '{print $1}') WORKLOAD=$(kubectl get po -n default \\ | grep \"example-\" | awk '{print $1}') INSPECTOR=$(kubectl get po -n default \\ | grep \"vsecm-inspector-\" | awk '{print $1}') # Delete secrets assigned to the workload: alias delete-secret=\"kubectl exec $SENTINEL \\ -n vsecm-system -- safe \\ -w example -s x -d\" alias inspect=\"kubectl exec $INSPECTOR -- ./env\" Now, we can start experimenting. Cleanup Let’s start with a blank slate again: delete-secret # Output: OK inspect # Output: # Failed to fetch the secrets. Try again later. # Secret does not exist The Format (-f) Argument VSecM Sentinel CLI accepts a format flag (-f), the possible values are \"json\" and \"yaml\" If it is not given, it defaults to \"json\"; however, in the upcoming examples we’ll be explicit and provide this argument at all times. Registering a JSON Secret kubectl exec $SENTINEL -n vsecm-system -- safe \\ -w example \\ -s '{\"username\": \"admin\", \"password\": \"VSecMRocks!\"}' \\ -f json inspect # Output: # {\"username\": \"admin\", \"password\": \"VSecMRocks!\"} Registering a YAML Secret kubectl exec $SENTINEL -n vsecm-system -- safe \\ -w example \\ -s '{\"username\": \"admin\", \"password\": \"VSecMRocks!\"}' \\ -f yaml inspect # Output: # password: VSecMRocks! # username: admin Registering a JSON String (with invalid JSON) Now we’ll deliberately make an error in our JSON. Notice the missing \" in username\": That is not valid JSON. kubectl exec $SENTINEL -n vsecm-system -- safe \\ -w example \\ -s '{username\": \"admin\", \"password\": \"VSecMRocks!\"}' \\ -f json inspect # Output: # {username\": \"admin\", \"password\": \"VSecMRocks!\"} Registering a YAML String (with invalid JSON) Since the JSON cannot be parsed, the output will not be a YAML: kubectl exec $SENTINEL -n vsecm-system -- safe \\ -w example \\ -s '{username\": \"admin\", \"password\": \"VSecMRocks!\"}' \\ -f yaml inspect # Output: # {username\": \"admin\", \"password\": \"VSecMRocks!\"} Transforming A JSON Secret kubectl exec $SENTINEL -n vsecm-system -- safe \\ -w example \\ -s '{\"username\": \"admin\", \"password\": \"VSecMRocks!\"}' \\ -t '{\"USR\":\"{{.username}}\", \"PWD\":\"{{.password}}\"}' \\ -f json inspect # Output: # {\"USR\":\"admin\", \"PWD\":\"VSecMRocks!\"} Transforming a YAML Secret kubectl exec $SENTINEL -n vsecm-system -- safe \\ -w example \\ -s '{\"username\": \"admin\", \"password\": \"VSecMRocks!\"}' \\ -t '{\"USR\":\"{{.username}}\", \"PWD\":\"{{.password}}\"}' \\ -f yaml inspect # Output: # PWD: VSecMRocks! # USR: admin Transforming a JSON Secret (invalid JSON) If our secret is not valid JSON, then the YAML transformation will not be possible. VMware Secrets Manager will still try its best to provide something. kubectl exec $SENTINEL -n vsecm-system -- safe \\ -w example \\ -s '{username\": \"admin\", \"password\": \"VSecMRocks!\"}' \\ -t '{\"USR\":\"{{.username}}\", \"PWD\":\"{{.password}}\"}' \\ -f json inspect # Output: # {username\": \"admin\", \"password\": \"VSecMRocks!\"} Transforming a JSON Secret (invalid template) Since template is not valid, the template transformation will not happen. kubectl exec $SENTINEL -n vsecm-system -- safe \\ -w example \\ -s '{\"username\": \"admin\", \"password\": \"VSecMRocks!\"}' \\ -t '{USR\":\"{{.username}}\", \"PWD\":\"{{.password}}\"}' \\ -f json inspect # Output: # {\"username\": \"admin\", \"password\": \"VSecMRocks!\"} Transforming a JSON Secret (invalid template and JSON) VMware Secrets Manager will still try its best: kubectl exec $SENTINEL -n vsecm-system -- safe \\ -w example \\ -s '{username\": \"admin\", \"password\": \"VSecMRocks!\"}' \\ -t '{USR\":\"{{.username}}\", \"PWD\":\"{{.password}}\"}' \\ -f json inspect # Output: # {username\": \"admin\", \"password\": \"VSecMRocks!\"} Transforming YAML Secret (invalid JSON) kubectl exec $SENTINEL -n vsecm-system -- safe \\ -w example \\ -s '{username\": \"admin\", \"password\": \"VSecMRocks!\"}' \\ -t '{\"USR\":\"{{.username}}\", \"PWD\":\"{{.password}}\"}' \\ -f yaml inspect # Output # {username\": \"admin\", \"password\": \"VSecMRocks!\"} Transforming YAML Secret (invalid template) kubectl exec $SENTINEL -n vsecm-system -- safe \\ -w example \\ -s '{\"username\": \"admin\", \"password\": \"VSecMRocks!\"}' \\ -t '{USR\":\"{{.username}}\", \"PWD\":\"{{.password}}\"}' \\ -f yaml inspect # Output: # {USR\":\"admin\", \"PWD\":\"VSecMRocks!\"} Transforming YAML Secret (invalid JSON and template) kubectl exec $SENTINEL -n vsecm-system -- safe \\ -w example \\ -s '{username\": \"admin\", \"password\": \"VSecMRocks!\"}' \\ -t '{USR\":\"{{.username}}\", \"PWD\":\"{{.password}}\"}' \\ -f yaml inspect # Output: # {username\": \"admin\", \"password\": \"VSecMRocks!\"} Conclusion This tutorial demonstrated various ways to transform and interpolate secret values into JSON and YAML. We also observed how the output is affected when there is a formatting issue with the secret, or the template to transform the secret, or both of them. The next section introduces a video tutorial that covers everything that has been mentioned so far and some more."
  },"/docs/showcase/": {
    "title": "VSecM Showcase",
    "keywords": "",
    "url": "/docs/showcase/",
    "body": "edit this page on GitHub ✏️ Introduction This page contains various usage examples and tutorials about VSecM. We add more videos here as we develop the project, enable more features, and cover more use cases. Enjoy, and may the source be with you. QuickStart Installation Building Blocks Local Deployment and Testing Configuration and Testing Options Using VSecM Sentinel Creating Helper Scripts and an Inspector Assigning Secrets to Workloads Encrypting Secrets Secret Template Transformations Interpolating VSecM Secrets to Kubernetes Secrets Aegis VMware Secrets Manager for Cloud-Native Apps is a successor of a project called Aegis. The project was renamed to VMware Secrets Manager and moved under the VMware Tanzu GitHub organization. In the meantime, you can check out the videos that relate to Aegis. Aside from the name, the videos are still relevant. Replace “Aegis” with “VSecM” When watching the videos remember to replace Aegis with VMware Secrets Manager, and the aegis command in Aegis Sentinel with the safe command in VSecM Sentinel. The rest of the content shall still be relevant. Without further ado, here is a playlist of videos that relate to Aegis:"
  },"/docs/changelog/": {
    "title": "Changelog",
    "keywords": "",
    "url": "/docs/changelog/",
    "body": "edit this page on GitHub ✏️ Recent Updates TBD [v0.21.4] - 2023-11-30 This patch release includes one security update, a minor refactoring, and documentation updates. Security This is a patch release to address GHSA-2c7c-3mj9-8fqh Decryption of malicious PBES2 JWE objects can consume unbounded system resources [v0.21.3] - 2023-11-03 Added Started experimental work on multi-cluster secret federation. Various Documentation updates. Automated Kubernetes manifest creation from Helm charts. Security Fixed GHSA-m425-mq94-257g gRPC-Go HTTP/2 Rapid Reset vulnerability [v0.21.2] - 2023-10-18 This is a purely security-focused release that fixes several vulnerabilities and also hardens the AES encryption flow against time-based attacks. Security Fixed CVE-2023-3978 Improper rendering of text nodes in golang.org/x/net/html Fixed CVE-2023-39325 HTTP/2 rapid reset can cause excessive work in net/http Fixed CVE-2023-44487 swift-nio-http2 vulnerable to HTTP/2 Stream Cancellation Attack Fixed an issue with possible memory overflow when doing a cryptographic size computation. Added a configurable throttle to AES IV computation to make it harder to perform time-based attacks. The computed AES IV is zeroed out after use for additional security. [v0.21.1] - 2023-10-11 Added Fixed spire-controller-manager’s version. The older setup was fixed on nightly which was causing ad-hoc issues. Changed Performance update: VSecM Sentinel now honors SIGTERM and SIGINT signals and gracefully shuts down when the pod is killed. Performance update: VSecM Safe is now leveraging several goroutines to speed up some of the blocking code paths during bootstrapping and initialization. Minor updates to the documentation. Security VSecM Safe has stricter validation routines for its identity. Added VSecM Keygen: a utility application that generates VSecM Safe’s bootstrapping keys if you want an extra level of security and control the creation of the master key. [v0.21.0] - 2023-09-08 Added Documentation updates to make the project align with the current status of VSecM. Migrate existing Aegis documentation to the new VMware Secrets Manager documentation site. Updated contributing guidelines to make it easier for first-time contributors. Published a formal project governance model. Added a blog section to the website. Decided to add a new helm chart per each release. Added instructional video content to the showcase section. Fixed Minor bugfixes after migration; ensuring feature and behavior parity with Aegis. Implemented stricter matchers for VSecM Sentinel and VSecM Safe’s Identity.yamls. Security Updated the security policy, clarifying our ideal response time for security vulnerabilities. Fixed a minor vulnerability in activesupport dependency: (CVE-2023-38037). fix; dependabot. The vulnerability affects only the website build process, not the VSecM codebase itself. It is not exploitable in our case, but we still wanted to fix it. [v0.20.0] - 2023-07-27 Added Migrated the source code from https://github.com/shieldworks/aegis to https://github.com/vmware-tanzu/secrets-manager Did necessary changes for the project to run build and pass tests. Created new container image repositories at https://hub.docker.com/u/vsecm. Changed Minor changes to build and deployment scripts. BREAKING: The binary that vsecm-sentinel uses is called safe right now (formerly it was aegis)."
  },"/docs/releases/": {
    "title": "Releases",
    "keywords": "",
    "url": "/docs/releases/",
    "body": "edit this page on GitHub ✏️ VMware Secrets Manager signs all of its releases using GitHub’s built-in signing process. We also sign our container images using Docker Content Trust. The following sections outline how you can verify the authenticity of our releases. Getting the Releases You can download the latest release from the GitHub Releases page. The related container images can be found on Docker Hub. Verifying Code Releases Our code releases are signed using GitHub’s built-in signing process. To verify a release: Clone the repository and navigate to it: git clone https://github.com/vmware-tanzu/secrets-manager.git cd secrets-manager Fetch the tags: git fetch --tags Verify the tag: git tag -v &lt;tag-name&gt; If the signature is valid, you will see a message confirming the signature check passed. Verifying Container Images We use Docker Content Trust to sign our Docker images. To verify the signature of an image, you can enable Docker Content Trust by setting the DOCKER_CONTENT_TRUST environment variable to 1. export DOCKER_CONTENT_TRUST=1 After enabling Docker Content Trust, any docker pull command will automatically verify the image signature before pulling it. docker pull vsecm/$yourImage # For, e.g.: docker pull vsecm/vsecm-ist-safe If the image signature is valid, the image will be pulled; otherwise, you will receive an error message."
  },"/docs/roadmap/": {
    "title": "Roadmap",
    "keywords": "",
    "url": "/docs/roadmap/",
    "body": "edit this page on GitHub ✏️ Introduction This is a page where we publish our approximate roadmap for VMware Secrets Manager for Cloud-Native Apps. Note that this is not a commitment to deliver any of the features listed here, and that the roadmap is subject to change at any time without notice. Whenever we release a new version of VMware Secrets Manager, we will update this page, and also the changelog to reflect the changes. One-Year Window This page will only contain information about the next 12 months of the project. We will update the roadmap every release, and remove the completed items from the list, and add a new iteration at the end of the list. Active Iterations VSecM v0.22.0 (codename: Boötes) Sep 12, 2023 – Jan 31, 2024 This is a relatively longer release because due to the “time-stop” effect of the holiday season, the majority of the core contributors will be spending quality time with their loved ones and recharging their batteries for the upcoming year. This release will be more about enhancing deployment workflows, testing automation and CI/CD pipelines. We will also focus on improving the overall user experience. Ability for an operator to export secrets (by providing a public key), to use in other workflows. Enhancements to template transformations. Better, machine-readable logs. Also, better audit logs. More documentation updates. Enabling automated tests and static code analysis. Preventing log tampering. Performance improvements on the website. VSecM v0.23.0 (codename: Cassiopeia) Oct 10, 2023 – Nov 6, 2023 This iteration will be focused on improving how VMware Secrets Manager logs and reports errors. We will also focus on improving the performance of the VMware Secrets Manager website. Secretless VSecM: Ability to use VMware Secrets Manager without relying on Kubernetes Secrets. This will allow users to use VMware Secrets Manager without having to create Kubernetes Secrets at all—even for the master keys. Ability to use VSecM across clusters (multi-cluster federation support). Static code analysis. More automation and tests. More use-case video lectures. Website enhancement: Versioned snapshots of the documentation. VSecM v0.24.0 (codename: Draco) Nov 7, 2023 – Dec 4, 2023 This iteration will be focused on making VMware Secrets Manager able to ingest large amounts of secrets, without crashing or slowing down. Stream manipulation: Ability to ingest large amounts of secrets; also ability to ingest longer secrets. More automation. Our goal is 90% code coverage by this checkpoint; or as far as we can get. Security: Ability to lock VSecM Safe. Documentation: A user guide for those who want to develop their own language-specific VSecM SDKs (we’ll continue to support Go only for a while, however we still want to make it easier for others to develop their own SDKs). Website optimization and asset cleanup. VSecM v0.25.0 (codename: Eridanus) Dec 5, 2023 – Jan 1, 2024 In this iteration, our focus will be in-memory usage of VSecM and also making the VSecM Sidecar more robust. Option for VSecM to run in-memory; without having to rely on any backing store. Option for the VSecM Sidecar to kill the container when the bound secret changes. Introducing Operators to auto-inject VSecM sidecars and init containers to workloads. VSecM v0.26.0 (codename: Fornax) Jan 2, 2024 – Jan 29, 2024 This is an iteration focused on code stability, and community development. Validation and guardrails around VSecM-managed SVIDs. Community development efforts. Focus on observability. VSecM v0.27.0 (codename: Gemini) Jan 30, 2024 – Feb 26, 2024 We’ll create abstractions around certain VMware Secrets Manager components to make further cloud integrations easier. Creating custom resources (ClusterVSecMId) for better abstraction. Improving usability and developer experience. VSecM v0.28.0 (codename: Hydra) Feb 27, 2024 – Mar 25, 2024 This iteration will be about providing access to VSecM Sentinel through OIDC authentication. We will also focus on various compatibility issues before we dive into cloud integration in the upcoming iterations. The goals in this iteration could be a stretch and based on the workload of the core maintainers, we might have to push some of these goals to the next iteration, thus impacting the overall roadmap. Focus on auto-scaling. OIDC authentication. Using Redis as a shared backing store. Ability to deploy VSecM to any SPIFFE-compatible cluster that has agents that provide SPIFFE Workload API. VSecM v0.29.0 (codename: Indus) Mar 26, 2024 – Apr 22, 2024 This iteration will be about integrating VMware Secrets Manager with AWS KMS. AWS KMS Integration VSecM v0.30.0 (codename: Lupus) Apr 23, 2024 – May 20, 2024 This iteration will be about integrating VMware Secrets Manager with Azure Key Vault. Azure Key Vault Integration VSecM v0.31.0 (codename: Mensa) May 21, 2024 – Jun 17, 2024 This release is about security auditing and hardening. Create a self-security assessment. Perform security audit from a third-party security firm, or an internal VMware security team. VSecM v0.32.0 (codename: Norma) Jun 18, 2024 – Jul 15, 2024 This iteration will be about integrating VMware Secrets Manager with Google Cloud KMS. Google Cloud KMS Integration VSecM v0.32.0 (codename: Orion) Jul 16, 2024 – Aug 12, 2024 This iteration will be about OIDC support, and improving our OpenSSF conformance. By this point, we expect to have at lest a silver, if not a gold badge from OpenSSF. We will also stretch our research into non-Kubernetes deployment options. Get an OpenSSF Silver Badge. Non-Kubernetes deployment options. Other integration options. Creating other official SDKs, along with VSecM Go SDK. Stability improvements. Closed Iterations VSecM v0.21.0 (codename: Andromeda) Aug 15, 2023 – Sep, 11, 2023 This was a stability-focused release. We focused on fixing bugs, improving stability, and improving workflows and CI/CD pipelines. We also created missing documentation and generated new video tutorials that feature the current version of VMware Secrets Manager. Check out the release notes to learn more about what has been added, changed, and fixed in this release."
  },"/docs/release-management/": {
    "title": "Release Management",
    "keywords": "",
    "url": "/docs/release-management/",
    "body": "edit this page on GitHub ✏️ Introduction This page discusses the release management process for VMware Secrets Manager. If you are responsible for cutting a release, please follow the steps outlined here. VMware Secrets Manager Build Server The VSecM Build Server Contains Trust Material The VSecM build server is a hardened and trusted environment with limited access. It contains trust material such as the Docker Content Trust root key, and the private key for signing the VSecM images. We (still) have a manual build process, so you will need access to the VSecM build server to be able to cut a release. You can of course build VSecM locally, but without the build server, you won’t be able to push the images to the registry and tag the release. Make Sure We Are Ready for a Release Cut Check out this internal link to see if there is any outstanding issues for the release. If they can be closed, close them. If they cannot be closed, move them to the next version. Make Sure You Update the Release Notes Add any publicly-known vulnerabilities that are fixed in this release. Add any significant changes completed to the release notes. Configuring Minikube Local Registry Switch to the $WORKSPACE/secrets-manager project folder Then, delete any existing minikube cluster. cd $WORKSPACE/secrets-manager make k8s-delete Then start the Minikube cluster. cd $WORKSPACE/secrets-manager make k8s-start This will also start the local registry. However, you will need to eval some environment variables to be able to use Minikube’s registry instead of the local Docker registry. cd $WORKSPACE/secrets-manager eval $(minikube docker-env) echo $DOCKER_HOST # example: tcp://192.168.49.2:2376 # # Any non-empty value to `echo $DOCKER_HOST` means that # the environment has been set up correctly. Creating a Local Deployment Follow these steps to build VSecM from scratch and deploy it to your local Minikube cluster, to experiment it with your workloads. # Temporarily disable Docker Content Trust # to deploy Minikube: export DOCKER_CONTENT_TRUST=0 make k8s-delete make k8s-start # The environment has changed; re-evaluate # the environment variables: eval $(minikube docker-env) make build-local make deploy-local When everything completes, you should be able to see VMware Secrets Manager pods in the vsecm-system namespace. kubectl get po -n vsecm-system # Output should list `vsecm-safe` and `vsecm-sentinel`. Cutting a Release Before every release cut, follow the steps outlined below. 0. Are you on a release branch? Make sure you are on a release branch, forked off of the most recent main branch. Also ensure that all changes have been merged to main. 1. Check Docker and Minikube Also make sure your docker and Minikube are up and running. Additionally, execute eval $(minikube -p minikube docker-env) once more to update your environment. 2. make help Check the make help command first, as it includes important information. You can also check make h command that included release-related commands. 3. Test VSecM Distroless Images VMware Secrets Manager Distroless series use lightweight and secure distroless images. make k8s-delete make k8s-start eval $(minikube -p minikube docker-env) # For macOS, you might need to run `make mac-tunnel` # on a separate terminal. # For other Linuxes, you might not need it. # # make mac-tunnel make build-local make deploy-local make test-local If the tests pass, go to the next step. 4. Test VSecM Photon (i.e. VMware Photon) Images VMware Secrets Manager Photon series use VMware Photon OS as their base images. make k8s-delete make k8s-start eval $(minikube -p minikube docker-env) # For macOS, you might need to run `make mac-tunnel` # on a separate terminal. # For other Linuxes, you might not need it. # # make mac-tunnel make build-local make deploy-photon-local make test-local 5. Test VSecM Distroless FIPS Images make k8s-delete make k8s-start eval $(minikube -p minikube docker-env) # For macOS, you might need to run `make mac-tunnel` # on a separate terminal. # For other Linuxes, you might not need it. # # make mac-tunnel make build-local make deploy-fips-local make test-local 6. Test VSecM Photon FIPS Images make k8s-delete make k8s-start eval $(minikube -p minikube docker-env) # For macOS, you might need to run `make mac-tunnel` # on a separate terminal. # For other Linuxes, you might not need it. # # make mac-tunnel make build-local make deploy-photon-fips-local make test-local 7. Tagging Tagging needs to be done on the build server. There is no automation for this yet. Don’t forget to Bump the Version If you are cutting a new release, do not forget to bump the version, before running the tagging script below. git checkout main git stash git pull export DOCKER_CONTENT_TRUST=1 make build make tag 8. Create a Release PR Since we successfully published a release to DockerHub, and ArtifactHub, we now can merge our changes to the main branch. Create a PR from the release branch you are on, and follow the regular merge approval process. 9. Tag the main Branch We have created a release in the previous step, but we did it on a feature branch. Now, we need to tag the main branch with the same version. Here’s an example: git tag -s v0.22.0-000 -a v0.22.0-000 git push origin --tags 10. Initializing Helm Charts To start the release cycle, we initialize helm-charts for each official release of VSecM. Helm-charts are continuously developed and updated during the release development process. At the beginning of a VSecM release, the init-next-helm-chart.sh script is used to initialize the helm-charts. To initialize a new helm-chart, run the following command using the init script: ./helm-charts/init_next_helm_chart.sh &lt;base-version&gt; &lt;new-version&gt; base-version: the existing helm-charts version to be used as the base helm-chart. new-version: the version helm-charts to be initialized. For example: ./helm-charts/init_next_helm_chart.sh 0.21.0 0.22.0 After execution, the script will display a link on the console. Use this link to create a pull request (PR) and merge it into the main branch. This will make the new helm-charts available for the VSecM release development cycle. 11. Release Helm Charts We offer the release_helm_chart.sh script for your use. To execute the script, provide the version of the helm-charts that you want to release as an argument. Use the following format: ./hack/release-helm-chart.sh &lt;version&gt; For example, to release version 0.22.0, run: ./hack/release-helm-chart.sh 0.22.0 Follow the instructions provided by the script for successful execution. Upon completion, the script will display a link on the console. Use this link to create a pull request (PR) and merge it into the gh-pages branch. 12. All Set 🎉 You’re all set. Happy releasing."
  },"/docs/governance/": {
    "title": "Governance",
    "keywords": "",
    "url": "/docs/governance/",
    "body": "VMware Secrets Manager Governance Model Introduction The governance model for VMware Secrets Manager is designed to be simple yet effective, allowing for clear decision-making and dispute resolution. The model is centralized, with Volkan Özçelik serving as the lead architect and benevolent dictator. Key Roles Lead Architect (Benevolent Dictator): Volkan Özçelik Makes all final decisions regarding the project. Responsible for architectural decisions and overall project direction. Contributors Anyone who contributes code, documentation, or other assets to the project. Can participate in discussions and code reviews. Decision-Making Process Proposal: Any contributor can propose changes or new features. Discussion: Proposals are open for discussion among all contributors. Review: Volkan Özçelik reviews the proposal and any discussions around it. Final Decision: Volkan Özçelik makes the final decision, taking into account the proposal, discussions, and the project’s goals. Dispute Resolution Open Discussion: Any disputes or disagreements should first be addressed through open discussion among contributors. Mediation: If no resolution is reached, Volkan Özçelik will mediate and provide insights. Final Ruling: Volkan Özçelik has the final say in resolving any disputes. Governance Models for Reference For more details on various governance models, you can refer to Governance Models."
  },"/docs/privacy/": {
    "title": "Privacy Policy",
    "keywords": "",
    "url": "/docs/privacy/",
    "body": "edit this page on GitHub ✏️ This is the shortest privacy policy you’ll possibly read. We hate writing boring copyright stuff, so please don’t make us write more. We Don’t Track No Cookies Wondering how this website fares when it comes to privacy and GDPR rules and regulations?—Good news: It does not use any tracking cookies. Your Personal Data is Safe We do not collect your data. Since we don’t have your data, we cannot share your data (d’uh). However, to make lawyers happy, we want to reiterate that we will never share your information with anyone without your prior consent. May the Source be With You 🦄 Have an ad-free, safe, and wonderful experience 🎉."
  },"/blog/keep-your-secrets/": {
    "title": "Keep Your Secrets… Secret",
    "keywords": "",
    "url": "/blog/keep-your-secrets/",
    "body": "Introducing VMware Secrets Manager In the ever-evolving landscape of cloud-native applications, secrets management is critical to ensuring sensitive information’s security and integrity. While several solutions are available, the recent shift in Hashicorp’s licensing towards a Business Source License (BSL) has raised concerns and sparked discussions within the community. As the lead architect of VMware Secrets Manager (VSecM), I’d like to take this opportunity to introduce our solution, which offers a robust, flexible, and permissive alternative to Hashicorp’s Vault. ⭐️ Star Us on GitHub ⭐️ If you find value in our approach and want to help others discover this outstanding technology, star our GitHub repository. Your support helps increase visibility and encourages more collaboration and innovation within the community. A New Licensing Landscape Hashicorp’s decision to adopt a Business Source License has been viewed by some as a restrictive move. The BSL, unlike open-source licenses, imposes certain limitations and conditions that may not align with the needs and values of all organizations and developers. In contrast, VMware Secrets Manager is licensed under the BSD 2-Clause License, a permissive open-source license that encourages collaboration, innovation, and freedom of use. VMware Secrets Manager: A Closer Look VSecM is designed with modern developers and DevOps professionals in mind. Here’s why it stands out: Ease of Use: With a Quickstart Guide, intuitive CLI, and developer-friendly SDK, VSecM simplifies secrets management without compromising security. Kubernetes Is a First-Class Citizen: VSecM seamlessly integrates with Kubernetes, leveraging SPIRE for authentication and offering flexible secret storage and transformation options. Community Engagement: We believe in open collaboration and community-driven development. Our Contributing Guide invites developers to get involved, ask questions, and contribute to the project. Secure by Default: VSecM offers advanced security features, including encryption, manual master secret setting, and integration with various backing stores. Join the Future of Secrets Management VMware Secrets Manager is more than just a tool; it’s a community-driven project that aims to redefine secrets management in a cloud-native world. We invite you to explore VSecM, contribute to its growth, and join us in shaping the future of secure and resilient applications. Whether you’re a developer looking to contribute or an organization seeking a flexible and secure solution for secrets management, VSecM offers a welcoming and innovative platform. This is Just the Beginning: A Vision for the Future At VMware Secrets Manager, we believe in continuous innovation and growth. Our roadmap is a testament to our commitment to delivering cutting-edge solutions that meet the evolving needs of the cloud-native community. Here’s a glimpse of what’s on the horizon: Stability and Usability Enhancements: Upcoming releases focus on improving stability, documentation, build automation, and overall user experience. Innovative Features: From Secretless VSecM to machine-readable logs, performance improvements, and large-scale secret ingestion, we’re pushing the boundaries of what’s possible in secrets management. Integration and Compatibility: Future iterations include integration with AWS KMS, Azure Key Vault, Google Cloud KMS, and even HashiCorp Vault, expanding the reach and compatibility of VSecM. Community Development and Collaboration: We are investing in community development efforts, multi-cluster secret federation, and creating abstractions to make cloud integrations easier. Our roadmap is not just a plan; it’s a promise to our users and contributors that we have a lot planned for the future of VMware Secrets Manager. We’re excited about the journey ahead and invite you to be a part of it. ⭐️ Star Us on GitHub ⭐️ If you find value in our approach and want to help others discover this outstanding technology, star our GitHub repository. Your support helps increase visibility and encourages more collaboration and innovation within the community. Help Us Shape the Future Start your journey with VMware Secrets Manager today and unlock the potential of cloud-native secrets management."
  }}
